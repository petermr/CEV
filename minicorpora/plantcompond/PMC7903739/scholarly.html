<html xmlns="http://www.w3.org/1999/xhtml"> <head> <style type="text/css">
      	     		 
      	    a                          {background : #ffffff; }
      	    article                    {border-style : dotted; border-width : 2px; }
		 	
		 	div                        {background : #ffffcc;}
		 	div.abstract-title         {font-weight : bold ; font-size : 16pt;}
		 	div.ack                    {border-style : solid ; border-color : red; margin : 2em; }
		 	div.article-meta           {border-style : solid; border-width : 1 pt; margin 1em;}
		 	div.boxed-text             {margin : 5em; border-style : solid;}
		 	div.contrib-group          {margin : 1em; }
		 	div.fig                    {border-style : solid; border-width : 2px; margin : 2em; }
		 	div.funding                {font-weight : bold ; font-size : 16pt;}
		 	div.given-names            {font-style : italic;}
		 	div.intro                  {border-style : inset; margin : 5px;}
		 	div.introduction           {border-style : inset; margin : 5px;}
		 	div.journal-meta           {border-style : solid; border-width : 1 pt; margin 1em;}
		 	div.journal-title-group    {background : #ffeeee;}
		 	div.article-title          {font-weight : bold ; font-size : 18pt;}
		 	div.kwd                    {font-style : italic}
		 	div.meta-name              {font-weight : bold ; font-size : 16pt;}
		 	div.name                   {font-weight : bold;}
		 	div.alt-title              {font-style : italic; font-size : 12px;}
		 	
		 	div.sec                    {border : 2px; margin 5px; padding 2px;}
		 	div.title                  {font-family : courier; font-weight : bold;
		 	                            font-size : 16pt; margin : 5px;}
		 	
		 	div.materials_methods      {border-style : double; margin : 5px; }
		 	div.methods                {border-style : double; margin : 5px; }
		 	
		 	div.results                {border-style : solid; margin : 5px;}
		 	div.background             {border-style : dotted; margin : 5px;}
		 	
		 	div.discussion             {border-style : groove; margin : 5px;}
		 	div.conclusion             {border-style : ridge; margin : 5px;}
		 	div.conclusions             {border-style : ridge; margin : 5px;}
		 	div.supplementary-material {border-style : inset; margin : 5px;}
		 	div.abbreviations          {border-style : double; border-color : red; margin : 5px;}
		 	div.competinginterests     {border-style : double; border-color : blue; margin : 5px;}
		 	div.acknowledgements       {border-style : double; border-color : green; margin : 5px;}
		 	div.authors_contributions   {border-style : double; border-color : purple; margin : 5px;}
		 	
		 	div.publisher              {border-style : outset; margin : 5px;}
		 	div.fn-type-conflict       {background : #f88; }
		 	div.fn-type-con            {background : #ddf; }
		 	div.fn-type-other          {background : #ddd; }
		 	
		 	div.unknown                {background : #ffd;
									 	  border-style : solid;
									 	  border-width : 1px;
									 	  padding : 2 px;}
		 	  
      	    table                      {background : #ffffdd;}
		 	tr                         {background : #ddddff; padding : 1px;}
		 	
		 	span                       {background : #ffcccc;}
		 	
		 	span.citation-author       {font-family : helvetica; background : #ffeeee;}
		 	span.collab                {background : #ddffff; }
		 	span.comment               {font-family : courier; font-size : 6px; background : #ffaaff;}
		 	span.contrib               {background : #ffffff;}
		 	span.corresp               {background : #ddffdd; }
		 	span.doi                   {background : #ffffff;}
		 	span.email                 {font-family : courier; }
		 	span.etal                  {font-style : italic;}
		 	span.fpage                 {font-family : courier;}
		 	span.given-names           {background : #ffffff;}
		 	span.iso-abbrev            {background : #ffffff;}
		 	span.issn-epub             {background : #ffffff;}
		 	span.issn-ppub             {background : #ffffff;}
		 	span.journal-title         {background : #ffffff;}
		 	span.lpage                 {font-family : courier;}
		 	span.mixed-article-title   {font-style : italic ;}
		 	span.nlm-ta                {background : #ffffff;}
		 	span.pmc                   {background : #ffffff;}
		 	span.pmcid                 {background : #ffffff;}
		 	span.pmid                  {background : #ffffff;}
		 	span.publisher             {background : #ffffff;}
		 	span.publisher-id          {background : #ffffff;}
		 	span.publisher-name        {background : #ffffff;}
		 	span.source                {background : #ffffff;}
		 	span.subject               {background : #ffffff;}
		 	span.surname               {background : #ffffff;}
		 	span.volume                {font-family : courier; font-weight : bold;}
		 	span.year                  {font-family : courier ; font-style : italic;}
			</style> </head> <body> <div class="front" title="front"> <div class="journal-meta" tagx="journal-meta" title="journal-meta"><span class="nlm-ta" title="nlm-ta">Plant Methods</span><span class="iso-abbrev" title="iso-abbrev">Plant Methods</span><div class="journal-title-group" tagx="journal-title-group" title="journal-title-group"><span class="journal-title" tagx="journal-title" title="journal-title">Plant Methods</span></div><span class="issn-epub" tagx="issn" title="issn-epub">1746-4811</span><div class="publisher" tagx="publisher" title="publisher"><span class="publisher-name" tagx="publisher-name" title="publisher-name">BioMed Central</span><span class="publisher-loc" tagx="publisher-loc" title="publisher-loc">London</span></div> </div> <div class="article-meta" tagx="article-meta" title="article-meta"><span class="pmcid" title="pmcid"> pmcid: <a href="http://www.ncbi.nlm.nih.gov/pubmed/7903739">7903739</a></span><span class="publisher-id" title="publisher-id">722</span><span class="doi" title="doi"> doi: <a href="https://dx.doi.org/10.1186/s13007-021-00722-9">10.1186/s13007-021-00722-9</a></span><div class="article-categories" title="article-categories">
: <span class="subject" title="subject">Review</span></div> <div class="title-group" tagx="title-group" title="title-group"> <div class="article-title" title="article-title">
Plant diseases and pests detection based on deep learning: a review</div> </div> <div class="contrib-group" title="contrib-group"><span class="contrib" title="contrib"><span class="contrib-id" tagx="contrib-id" title="contrib-id">http://orcid.org/0000-0001-8769-5981</span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Liu</span><span class="given-names" tagx="given-names" title="given-names">Jun</span></span><span class="address" title="address"><span class="email" tagx="email" title="email">liu_jun860116@wfust.edu.cn</span></span><a href="#Aff1" /></span><span class="contrib" title="contrib"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wang</span><span class="given-names" tagx="given-names" title="given-names">Xuewei</span></span><span class="address" title="address"><span class="email" tagx="email" title="email">wangxuewei@wfust.edu.cn</span></span><a href="#Aff1" /></span><span class="citation_author_institution" id="Aff1">[], <div class="institution-wrap" title="institution-wrap"><span class="institution-id" title="institution-id">grid.460150.6</span><span class="institution-id" title="institution-id">0000 0004 1759 7077</span><span class="institution" title="institution">Shandong Provincial University Laboratory for Protected Horticulture, Blockchain Laboratory of Agricultural Vegetables, </span><span class="institution" title="institution">Weifang University of Science and Technology, </span></div></span></div><span class="pub-date-epub" title="pub-date-epub">epub: <span>2021-2-2</span></span><span class="pub-date-pmc-release" title="pub-date-pmc-release">pmc-release: <span>2021-2-2</span></span><span class="pub-date-collection" title="pub-date-collection">collection: <span>2021</span></span><span class="volume" tagx="volume" title="volume">17</span><span class="elocation-id" tagx="elocation-id" title="elocation-id">22</span><span class="history" title="history"><span class="received" title="received">received: 2020-9-20</span><span class="accepted" title="accepted">accepted: 2021-2-13</span></span><div class="permissions"><span class="copyright" title="copyright">(C) , </span><span class="license" title="license"><span class="license-p" title="license-p"><b>Open Access</b>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</a>. The Creative Commons Public Domain Dedication waiver (<a href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</a>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</span></span></div> <div class="abstract" title="abstract"> <div class="abstract-title" title="abstract-title">
Abstract</div> <p id="Par1">Plant diseases and pests are important factors determining the yield and quality of plants. Plant diseases and pests identification can be carried out by means of digital image processing. In recent years, deep learning has made breakthroughs in the field of digital image processing, far superior to traditional methods. How to use deep learning technology to study plant diseases and pests identification has become a research issue of great concern to researchers. This review provides a definition of plant diseases and pests detection problem, puts forward a comparison with traditional plant diseases and pests detection methods. According to the difference of network structure, this study outlines the research on plant diseases and pests detection based on deep learning in recent years from three aspects of classification network, detection network and segmentation network, and the advantages and disadvantages of each method are summarized. Common datasets are introduced, and the performance of existing studies is compared. On this basis, this study discusses possible challenges in practical applications of plant diseases and pests detection based on deep learning. In addition, possible solutions and research ideas are proposed for the challenges, and several suggestions are given. Finally, this study gives the analysis and prospect of the future trend of plant diseases and pests detection based on deep learning.</p> </div> <div class="kwd-group"> <div class="title" tagx="title" title="title">
Keywords</div> <div class="kwd" title="kwd">
Deep learning</div> <div class="kwd" title="kwd">
Convolutional neural network</div> <div class="kwd" title="kwd">
Plant diseases and pests</div> <div class="kwd" title="kwd">
Classification</div> <div class="kwd" title="kwd">
Object detection</div> <div class="kwd" title="kwd">
Segmentation</div> </div> <div class="custom-meta-group" title="custom-meta-group"> <meta name="issue-copyright-statement" value="Â© The Author(s) 2021" /> </div> </div> </div> <div class="body" title="body"> <div class="background" title="sec"> <div class="title" tagx="title" title="title">
Background</div> <p id="Par2">Plant diseases and pests detection is a very important research content in the field of machine vision. It is a technology that uses machine vision equipment to acquire images to judge whether there are diseases and pests in the collected plant images [<a href="#CR1">1</a>]. At present, machine vision-based plant diseases and pests detection equipment has been initially applied in agriculture and has replaced the traditional naked eye identification to some extent.</p> <p id="Par3">For traditional machine vision-based plant diseases and pests detection method, conventional image processing algorithms or manual design of features plus classifiers are often used [<a href="#CR2">2</a>]. This kind of method usually makes use of the different properties of plant diseases and pests to design the imaging scheme and chooses appropriate light source and shooting angle, which is helpful to obtain images with uniform illumination. Although carefully constructed imaging schemes can greatly reduce the difficulty of classical algorithm design, but also increase the application cost. At the same time, under natural environment, it is often unrealistic to expect the classical algorithms designed to completely eliminate the impact of scene changes on the recognition results [<a href="#CR3">3</a>]. In real complex natural environment, plant diseases and pests detection is faced with many challenges, such as small difference between the lesion area and the background, low contrast, large variations in the scale of the lesion area and various types, and a lot of noise in the lesion image. Also, there are a lot of disturbances when collecting plant diseases and pests images under natural light conditions. At this time, the traditional classical methods often appear helpless, and it is difficult to achieve better detection results.</p> <p id="Par4">In recent years, with the successful application of deep learning model represented by convolutional neural network (CNN) in many fields of computer vision (CV, computer-vision), for example, traffic detection [<a href="#CR4">4</a>], medical Image Recognition [<a href="#CR5">5</a>], Scenario text detection [<a href="#CR6">6</a>], expression recognition [<a href="#CR7">7</a>], face Recognition [<a href="#CR8">8</a>], etc. Several plant diseases and pests detection methods based on deep learning are applied in real agricultural practice, and some domestic and foreign companies have developed a variety of deep learning-based plant diseases and pests detection Wechat applet and photo recognition APP software. Therefore, plant diseases and pests detection method based on deep learning not only has important academic research value, but also has a very broad market application prospect.</p> <p id="Par5">In view of the lack of comprehensive and detailed discussion on plant diseases and pests detection methods based on deep learning, this study summarizes and combs the relevant literatures from 2014 to 2020, aiming to help researchers quickly and systematically understand the relevant methods and technologies in this field. The content of this study is arranged as follows: â€œ<a href="#Sec2">Definition of plant diseases and pests detection problem</a>â€� section gives the definition of plant diseases and pests detection problem; â€œ<a href="#Sec6">Image recognition technology based on deep learning</a>â€� section focuses on the detailed introduction of image recognition technology based on deep learning; â€œ<a href="#Sec10">Plant diseases and pests detection methods based on deep learning</a>â€� section analyses the three kinds of plant diseases and pests detection methods based on deep learning according to network structure, including classification, detection and segmentation network; â€œ<a href="#Sec21">Dataset and performance comparison</a>â€� section introduces some datasets of plant diseases and pests detection and compares the performance of the existing studies; â€œ<a href="#Sec25">Challenges</a>â€� section puts forward the challenges of plant diseases and pests detection based on deep learning; â€œ<a href="#Sec37">Conclusions and future directions</a>â€� section prospects the possible research focus and development direction in the future.</p> </div> <div class="definitionofplantdiseasesandpestsdetectionproblem" title="sec"> <div class="title" tagx="title" title="title">
Definition of plant diseases and pests detection problem</div> <div class="definitionofplantdiseasesandpests" title="sec"> <div class="title" tagx="title" title="title">
Definition of plant diseases and pests</div> <p id="Par6">Plant diseases and pests is one kind of natural disasters that affect the normal growth of plants and even cause plant death during the whole growth process of plants from seed development to seedling and to seedling growth. In machine vision tasks, plant diseases and pests tend to be the concepts of human experience rather than a purely mathematical definition.</p> </div> <div class="definitionofplantdiseasesandpestsdetection" title="sec"> <div class="title" tagx="title" title="title">
Definition of plant diseases and pests detection</div> <p id="Par7">Compared with the definite classification, detection and segmentation tasks in computer vision [<a href="#CR9">9</a>], the requirements of plant diseases and pests detection is very general. In fact, its requirements can be divided into three different levels: what, where and how [<a href="#CR10">10</a>]. In the first stage, â€œwhatâ€� corresponds to the classification task in computer vision. As shown in Fig.Â <a href="#Fig1">1</a>, the label of the category to which it belongs is given. The task in this stage can be called classification and only gives the category information of the image. In the second stage, â€œwhereâ€� corresponds to the location task in computer vision, and the positioning of this stage is the rigorous sense of detection. This stage not only acquires what types of diseases and pests exist in the image, but also gives their specific locations. As shown in Fig.Â <a href="#Fig1">1</a>, the plaque area of gray mold is marked with a rectangular box. In the third stage, â€œhowâ€� corresponds to the segmentation task in computer vision. As shown in Fig.Â <a href="#Fig1">1</a>, the lesions of gray mold are separated from the background pixel by pixel, and a series of information such as the length, area, location of the lesions of gray mold can be further obtained, which can assist the higher-level severity level evaluation of plant diseases and pests. Classification describes the image globally through feature expression, and then determines whether there is a certain kind of object in the image by means of classification operation; while object detection focuses on local description, that is, answering what object exists in what position in an image, so in addition to feature expression, object structure is the most obvious feature that object detection differs from object classification. That is, feature expression is the main research line of object classification, while structure learning is the research focus of object detection. Although the function requirements and objectives of the three stages of plant diseases and pests detection are different, yet in fact, the three stages are mutually inclusive and can be converted. For example, the â€œwhereâ€� in the second stage contains the process of â€œwhatâ€� in the first stage, and the â€œhowâ€� in the third stage can finish the task of â€œwhereâ€� in the second stage. Also, the â€œwhatâ€� in the first stage can achieve the goal of the second and the third stages through some methods. Therefore, the problem in this study is collectively referred to as plant diseases and pests detection as conventions in the following text, and the terminology differentiates only when different network structures and functions are adopted. </p><div class="fig" title="fig"> <div class="figure" title="figure"><a href="http://doi.org/"><span class="label" tagx="label" title="label">Fig. 1</span></a></div>   <p>Definition of plant diseases and pests detection problem</p>   </div> <p /> </div> <div class="comparisonwithtraditionalplantdiseasesandpestsdetectionmethods" title="sec"> <div class="title" tagx="title" title="title">
Comparison with traditional plant diseases and pests detection methods</div> <p id="Par8">To better illustrate the characteristics of plant diseases and pests detection methods based on deep learning, according to existing references [<a href="#CR11">11</a>â€"<a href="#CR15">15</a>], a comparison with traditional plant diseases and pests detection methods is given from four aspects including essence, method, required conditions and applicable scenarios. Detailed comparison results are shown in Table <a href="#Tab1">1</a>. <div class="table-wrap_UNKNOWN" id="Tab1"><span class="label" tagx="label" title="label">Table 1</span> </div></p><p>Contrast between traditional image processing methods and deep learning methods</p>  <table frame="hsides" rules="groups"> <thead> <tr> <th align="left">Technology</th> <th align="left">Traditional image processing methods</th> <th align="left">Deep learning methods</th> </tr> </thead> <tbody> <tr> <td align="left">Essence</td> <td align="left">Manual design featuresâ€‰+â€‰classifiers (or rules)</td> <td align="left">Automatic learning of features from large amounts of data</td> </tr> <tr> <td align="left">Method</td> <td align="left"> <p>Image segmentation method: Threshold segmentation; Roberts, Prewitt, Sobel, Laplace and Kirsh edge detection; region segmentation</p> <p>Feature extraction method: SIFT, HOG, LBP, shape, color and texture feature extraction method</p> <p>Classification method: SVM, BP, Bayesian</p> </td> <td align="left">CNN</td> </tr> <tr> <td align="left">Required conditions</td> <td align="left">Relatively harsh imaging environment requirements, high contrast between lesion and non-lesion areas, less noise</td> <td align="left">Adequate learning data and high-performance computing units</td> </tr> <tr> <td align="left">Applicable scenarios</td> <td align="left">It is often necessary to change the threshold or redesign the algorithm when imaging environment or plant diseases and pests class changes, which has poor recognition effect in real complex natural environment</td> <td align="left">It has ability to cope with certain real and complex natural environment changes</td> </tr> </tbody> </table>  <p /> </div> </div> <div class="imagerecognitiontechnologybasedondeeplearning" title="sec"> <div class="title" tagx="title" title="title">
Image recognition technology based on deep learning</div> <p id="Par9">Compared with other image recognition methods, the image recognition technology based on deep learning does not need to extract specific features, and only through iterative learning can find appropriate features, which can acquire global and contextual features of images, and has strong robustness and higher recognition accuracy.</p> <div class="deeplearningtheory" title="sec"> <div class="title" tagx="title" title="title">
Deep learning theory</div> <p id="Par10">The concept of Deep Learning (DL) originated from a paper published in Science by Hinton et al. [<a href="#CR16">16</a>] in 2006. The basic idea of deep learning is: using neural network for data analysis and feature learning, data features are extracted by multiple hidden layers, each hidden layer can be regarded as a perceptron, the perceptron is used to extract low-level features, and then combine low-level features to obtain abstract high-level features, which can significantly alleviate the problem of local minimum. Deep learning overcomes the disadvantage that traditional algorithms rely on artificially designed features and has attracted more and more researchersâ€™ attention. It has now been successfully applied in computer vision, pattern recognition, speech recognition, natural language processing and recommendation systems [<a href="#CR17">17</a>].</p> <p id="Par11">Traditional image classification and recognition methods of manual design features can only extract the underlying features, and it is difficult to extract the deep and complex image feature information [<a href="#CR18">18</a>]. And deep learning method can solve this bottleneck. It can directly conduct unsupervised learning from the original image to obtain multi-level image feature information such as low-level features, intermediate features and high-level semantic features. Traditional plant diseases and pests detection algorithms mainly adopt the image recognition method of manual designed features, which is difficult and depends on experience and luck, and cannot automatically learn and extract features from the original image. On the contrary, deep learning can automatically learn features from large data without manual manipulation. The model is composed of multiple layers, which has good autonomous learning ability and feature expression ability, and can automatically extract image features for image classification and recognition. Therefore, deep learning can play a great role in the field of plant diseases and pests image recognition. At present, deep learning methods have developed many well-known deep neural network models, including deep belief network (DBN), deep Boltzmann machine (DBM), stack de-noising autoencoder (SDAE) and deep convolutional neural network (CNN) [<a href="#CR19">19</a>]. In the area of image recognition, the use of these deep neural network models to realize automate feature extraction from high-dimensional feature space offers significant advantages over traditional manual design feature extraction methods. In addition, as the number of training samples grows and the computational power increases, the characterization power of deep neural networks is being further improved. Nowadays, the boom of deep learning is sweeping both industry and academia, and the performance of deep neural network models are all significantly ahead of traditional models. In recent years, the most popular deep learning framework is deep convolutional neural network.</p> </div> <div class="convolutionalneuralnetwork" title="sec"> <div class="title" tagx="title" title="title">
Convolutional neural network</div> <p id="Par12">Convolutional Neural Networks, abbreviated as CNN, has a complex network structure and can perform convolution operations. As shown in Fig.Â <a href="#Fig2">2</a>, the convolutional neural network model is composed of input layer, convolution layer, pooling layer, full connection layer and output layer. In one model, the convolution layer and the pooling layer alternate several times, and when the neurons of the convolution layer are connected to the neurons of the pooling layer, no full connection is required. CNN is a popular model in the field of deep learning. The reason lies in the huge model capacity and complex information brought about by the basic structural characteristics of CNN, which enables CNN to play an advantage in image recognition. At the same time, the successes of CNN in computer vision tasks have boosted the growing popularity of deep learning. </p><div class="fig" title="fig"> <div class="figure" title="figure"><a href="http://doi.org/"><span class="label" tagx="label" title="label">Fig. 2</span></a></div>   <p>The basic structure of CNN</p>   </div> <p /> <p id="Par13">In the convolution layer, a convolution core is defined first. The convolution core can be considered as a local receptive field, and the local receptive field is the greatest advantage of the convolution neural network. When processing data information, the convolution core slides on the feature map to extract part of the feature information. After the feature extraction of the convolution layer, the neurons are input into the pooling layer to extract the feature again. At present, the commonly used methods of pooling include calculating the mean, maximum and random values of all values in the local receptive field [<a href="#CR20">20</a>, <a href="#CR21">21</a>]. After the data entering several convolution layers and pooling layers, they enter the full-connection layer, and the neurons in the full-connection layer are fully connected with the neurons in the upper layer. Finally, the data in the full-connection layer can be classified by the softmax method, and then the values are transmitted to the output layer for output results.</p> </div> <div class="opensourcetoolsfordeeplearning" title="sec"> <div class="title" tagx="title" title="title">
Open source tools for deep learning</div> <p id="Par14">The commonly used third-party open source tools for deep learning are Tensorflow [<a href="#CR22">22</a>], Torch/PyTorch [<a href="#CR23">23</a>], Caffe [<a href="#CR24">24</a>], Theano [<a href="#CR25">25</a>]. The different characteristics of each open source tool are shown in Table <a href="#Tab2">2</a>. <div class="table-wrap_UNKNOWN" id="Tab2"><span class="label" tagx="label" title="label">Table 2</span> </div></p><p>Comparison of open source tools for deep learning</p>  <table frame="hsides" rules="groups"> <thead> <tr> <th align="left">Tools</th> <th align="left">Publisher</th> <th align="left">Supporting hardware</th> <th align="left">Applicable interface</th> <th align="left">Usability</th> </tr> </thead> <tbody> <tr> <td align="left">Tensorflow</td> <td align="left">Google</td> <td align="left">CPU, GPU, Mobile</td> <td align="left">C, Python</td> <td align="left">Flexible development, portability, powerful performance, support for distributed applications</td> </tr> <tr> <td align="left">Torch/PyTorch</td> <td align="left">Facebook</td> <td align="left">CPU, GPU, FPGA</td> <td align="left">C, Python, Lua</td> <td align="left">Easy to debug and develop, support dynamic neural network, easy to expand, modularization and low learning cost</td> </tr> <tr> <td align="left">Caffe</td> <td align="left">BAIR</td> <td align="left">CPU, GPU</td> <td align="left">Python, Matlab</td> <td align="left">High readability, easy to expand, fast speed, large number of users and wide community</td> </tr> <tr> <td align="left">Theano</td> <td align="left">MILA</td> <td align="left">CPU, GPU</td> <td align="left">Python</td> <td align="left">Flexible and high performance</td> </tr> </tbody> </table>  <p /> <p id="Par15">The four commonly used deep learning third-party open source tools all support cross-platform operation, and the platforms that can be run include Linux, Windows, iOS, Android, etc. Torch/PyTorch and Tensorflow have good scalability and support a large number of third-party libraries and deep network structures, and have the fastest training speed when training large CNN networks on GPU.</p> </div> </div> <div class="plantdiseasesandpestsdetectionmethodsbasedondeeplearning" title="sec"> <div class="title" tagx="title" title="title">
Plant diseases and pests detection methods based on deep learning</div> <p id="Par16">This section gives a summary overview of plant diseases and pests detection methods based on deep learning. Since the goal achieved is completely consistent with the computer vision task, plant diseases and pests detection methods based on deep learning can be seen as an application of relevant classical networks in the field of agriculture. As shown in Fig.Â <a href="#Fig3">3</a>, the network can be further subdivided into classification network, detection network and segmentation network according to the different network structures. As can be seen from Fig.Â <a href="#Fig3">3</a>, this paper is subdivided into several different sub-methods according to the processing characteristics of each type of methods. </p><div class="fig" title="fig"> <div class="figure" title="figure"><a href="http://doi.org/"><span class="label" tagx="label" title="label">Fig. 3</span></a></div>   <p>Framework of plant diseases and pests detection methods based on deep learning</p>   </div> <p /> <div class="classificationnetwork" title="sec"> <div class="title" tagx="title" title="title">
Classification network</div> <p id="Par17">In real natural environment, the great differences in shape, size, texture, color, background, layout and imaging illumination of plant diseases and pests make the recognition a difficult task. Due to the strong feature extraction capability of CNN, the adoption of CNN-based classification network has become the most commonly used pattern in plant diseases and pests classification. Generally, the feature extraction part of CNN classification network consists of cascaded convolution layerâ€‰+â€‰pooling layer, followed by full connection layer (or average pooling layer)â€‰+â€‰softmax structure for classification. Existing plant diseases and pests classification network mostly use the muture network structures in computer vision, including AlexNet [<a href="#CR26">26</a>], GoogleLeNet [<a href="#CR27">27</a>], VGGNet [<a href="#CR28">28</a>], ResNet [<a href="#CR29">29</a>], Inception V4 [<a href="#CR30">30</a>], DenseNets [<a href="#CR31">31</a>], MobileNet [<a href="#CR32">32</a>] and SqueezeNet [<a href="#CR33">33</a>]. There are also some studies which have designed network structures based on practical problems [<a href="#CR34">34</a>â€"<a href="#CR37">37</a>]. By inputting a test image into the classification network, the network analyses the input image and returns a label that classifies the image. According to the difference of tasks achieved by the classification network method, it can be subdivided into three subcategories: using the network as a feature extractor, using the network for classification directly and using the network for lesions location.</p> <div class="usingnetworkasfeatureextractor" title="sec"> <div class="title" tagx="title" title="title">
Using network as feature extractor</div> <p id="Par18">In the early studies on plant diseases and pests classification methods based on deep learning, many researchers took advantage of the powerful feature extraction capability of CNN, and the methods were combined with traditional classifiers [<a href="#CR38">38</a>]. First, the images are input into a pretrained CNN network to obtain image characterization features, and the acquired features are then input into a conventional machine learning classifier (e.g., SVM) for classification. Yalcin et al. [<a href="#CR39">39</a>] proposed a convolutional neural network architecture to extract the features of images while performing experiments using SVM classifiers with different kernels and feature descriptors such as LBP and GIST, the experimental results confirmed the effectiveness of the approach. Fuentes et al. [<a href="#CR40">40</a>] put forward the idea of CNN based meta architecture with different feature extractors, and the input images included healthy and infected plants, which were identified as their respective classes after going through the meta architecture. Hasan et al. [<a href="#CR41">41</a>] identified and classified nine different types of rice diseases by using the features extracted from DCNN model and input into SVM, and the accuracy achieved 97.5%.</p> </div> <div class="usingnetworkforclassificationdirectly" title="sec"> <div class="title" tagx="title" title="title">
Using network for classification directly</div> <p id="Par19">Directly using classification network to classify lesions is the earliest common means of CNN applied in plant diseases and pests detection. According to the characteristics of existing research work, it can be further subdivided into original image classification, classification after locating Region of Interest (ROI) and multi-category classification. </p><ol> <li> <p id="Par20">Original image classification. That is, directly put the collected complete plant diseases and pests image into the network for learning and training. Thenmozhi et al. [<a href="#CR42">42</a>] proposed an effective deep CNN model, and transfer learning is used to fine-tune the pre-training model. Insect species were classified on three public insect datasets with accuracy of 96.75%, 97.47% and 95.97%, respectively. Fang et al. [<a href="#CR43">43</a>] used ResNet50 in plant diseases and pests detection. The focus loss function was used instead of the standard cross-entropy loss function, and the Adam optimization method was used to identify the leaf disease grade, and the accuracy achieved 95.61%.</p> </li> <li> <p id="Par21">Classification after locating ROI. For the whole image acquired, we should focus on whether there is a lesion in a fixed area, so we often obtain the region of interest (ROI) in advance, and then input the ROI into the network to judge the category of diseases and pests. Nagasubramanian et al. [<a href="#CR44">44</a>] used a new three-dimensional deep convolution neural network (DCNN) and salience map visualization method to identify healthy and infected samples of soybean stem rot, and the classification accuracy achieved 95.73%.</p> </li> <li> <p id="Par22">Multi-category classification. When the number of plant diseases and pests class to be classified exceed 2 class, the conventional plant diseases and pests classification network is the same as the original image classification method, that is, the output nodes of the network are the number of plant diseases and pests classâ€‰+â€‰1 (including normal class). However, multi-category classification methods often use a basic network to classify lesions and normal samples, and then share feature extraction parts on the same network to modify or increase the classification branches of lesion categories. This approach is equivalent to preparing a pre-training weight parameter for subsequent multi-objective plant diseases and pests classification network, which is obtained by binary training between normal samples and plant diseases and pests samples. Picon et al. [<a href="#CR45">45</a>] proposed a CNN architecture to identify 17 diseases in 5 crops, which seamlessly integrates context metadata, allowing training of a single multi-crop model. The model can achieve the following goals: (a) obtains richer and more robust shared visual features than the corresponding single crop; (b) is not affected by different diseases in which different crops have similar symptoms; (c) seamlessly integrates context to perform crop conditional disease classification. Experiments show that the proposed model alleviates the problem of data imbalance, and the average balanced accuracy is 0.98, which is superior to other methods and eliminates 71% of classifier errors.</p> </li> </ol> <p /> </div> <div class="usingnetworkforlesionslocation" title="sec"> <div class="title" tagx="title" title="title">
Using network for lesions location</div> <p id="Par23">Generally, the classification network can only complete the classification of image label level. In fact, it can also achieve the location of lesions and the pixel-by-pixel classification by combining different techniques and methods. According to the different means used, it can be further divided into three forms: sliding window, heatmap and multi-task learning network. </p><ol> <li> <p id="Par24">Sliding window. This is the simplest and intuitive method to achieve the location of lesion coarsely. The image in the sliding window is input into the classification network for plant diseases and pests detection by redundant sliding on the original image through a smaller size window. Finally, all sliding windows are connected to obtain the results of the location of lesion. Chen et al. [<a href="#CR46">46</a>] used CNN classification network based on sliding window to build a framework for characteristics automatic learning, feature fusion, recognition and location regression calculation of plant diseases and pests species, and the recognition rate of 38 common symptoms in the field was 50â€"90%.</p> </li> <li> <p id="Par25">Heatmap. This is an image that reflects the importance of each region in the image, the darker the color represents the more important. In the field of plant diseases and pests detection, the darker the color in the heatmap represents the greater the probability that it is the lesion. In 2017, Dechant et al. [<a href="#CR47">47</a>] trained CNN to make heatmap to show the probability of infection in each region in maize disease images, and these heatmaps were used to classify the complete images, dividing each image into containing or not containing infected leaves. At runtime, it takes about 2Â min to generate a heatmap for an image (1.6Â GB of memory) and less than one second to classify a set of three heatmaps (800Â MB of memory). Experiments show that the accuracy is 96.7% on the test dataset. In 2019, Wiesner-Hanks et al. [<a href="#CR48">48</a>] used heatmap method to obtain accurate contour areas of maize diseases, the model can accurately depict lesions as low as millimeter scale from the images collected by UAVs, with an accuracy rate of 99.79%, which is the best scale of aerial plant disease detection achieved so far.</p> </li> <li> <p id="Par26">Multi-task learning network. If the pure classified network does not add any other skills, it could only realize the image level classification. Therefore, to accurately locate the location of plant diseases and pests, the designed network should often add an extra branch, and the two branches would share the results of the feature extracting. In this way, the network generally had the classification and segmentation output of the plant diseases and pests, forming a multi-task learning network. It takes into account the characteristics of both network. For segmentation network branches, each pixel in the image can be used as a training sample to train the network. Therefore, the multi-task learning network not only uses the segmentation branches to output the specific segmentation results of the lesions, but also greatly reduces the requirements of the classification network for samples. Ren et al. [<a href="#CR49">49</a>] constructed a Deconvolution-Guided VGNet (DGVGNet) model to identify plant leaf diseases which were easily disturbed by shadows, occlusions and light intensity. The deconvolution was used to guide the CNN classifier to focus on the real lesion sites. The test results show that the accuracy of disease class identification is 99.19%, the pixel accuracy of lesion segmentation is 94.66%, and the model has good robustness in occlusion, low light and other environments.</p> </li> </ol> <p /> <p id="Par27">To sum up, the method based on classification network is widely used in practice, and many scholars have carried out application research on the classification of plant diseases and pests [<a href="#CR50">50</a>â€"<a href="#CR53">53</a>]. At the same time, different sub-methods have their own advantages and disadvantages, as shown in Table <a href="#Tab3">3</a>. <div class="table-wrap_UNKNOWN" id="Tab3"><span class="label" tagx="label" title="label">Table 3</span> </div></p><p>Comparison of advantages and disadvantages of each sub-method of classification network</p>  <table frame="hsides" rules="groups"> <thead> <tr> <th align="left">Method</th> <th align="left">Advantages</th> <th align="left">Disadvantages</th> </tr> </thead> <tbody> <tr> <td align="left">Using network as feature extractor</td> <td align="left">Obtaining effective lesion features</td> <td align="left">Relying on other classifiers for final classification results</td> </tr> <tr> <td align="left">Original image classification</td> <td align="left">Classic in structure, it is also the basis of other classification network sub-methods and can refer to many existing networks</td> <td align="left">Lesions need to account for a certain proportion in the image, otherwise their characteristics are easily pooled out, and generally only one class of lesion is allowed in an image</td> </tr> <tr> <td align="left">Classification after locating ROI</td> <td align="left">Obtaining ROI information of the lesions</td> <td align="left">Additional methods are needed to obtain ROI</td> </tr> <tr> <td align="left">Multi-category classification</td> <td align="left">Solving sample imbalance to some extent</td> <td align="left">Secondary training is needed</td> </tr> <tr> <td align="left">Sliding window</td> <td align="left">Get rough localization of lesions in images</td> <td align="left">Sliding window size requires accurate selection, and can only get rough position, slow speed of traversal and sliding</td> </tr> <tr> <td align="left">Heatmap</td> <td align="left">Generate more accurate lesion areas</td> <td align="left">Accurate lesions location depends on network classification performance</td> </tr> <tr> <td align="left">Multi-task learning network</td> <td align="left">Combining other networks to obtain exact location and category of lesions simultaneously, and reducing the number of training samples required</td> <td align="left">The network structure is relatively complex, and a pixel-by-pixel label is required when adding segmentation branches</td> </tr> </tbody> </table>  <p /> </div> </div> <div class="detectionnetwork" title="sec"> <div class="title" tagx="title" title="title">
Detection network</div> <p id="Par28">Object positioning is one of the most basic tasks in the field of computer vision. It is also the closest task to plant diseases and pests detections in the traditional sense. Its purpose is to obtain accurate location and category information of the object. At present, object detection methods based on deep learning emerge endlessly. Generally speaking, plant diseases and pests detection network based on deep learning can be divided into: two stage network represented by Faster R-CNN [<a href="#CR54">54</a>]; one stage network represented by SSD [<a href="#CR55">55</a>] and YOLO [<a href="#CR56">56</a>â€"<a href="#CR58">58</a>]. The main difference between the two networks is that the two-stage network needs to first generate a candidate box (proposal) that may contain the lesions, and then further execute the object detection process. In contrast, the one-stage network directly uses the features extracted in the network to predict the location and class of the lesions.</p> <div class="plantdiseasesandpestsdetectionbasedontwostagesnetwork" title="sec"> <div class="title" tagx="title" title="title">
Plant diseases and pests detection based on two stages network</div> <p id="Par29">The basic process of two-stage detection network (Faster R-CNN) is to obtain the feature map of the input image through the backbone network first, then calculate the anchor box confidence using RPN and get the proposal. Then, input the feature map of the proposal area after ROIpooling to the network, fine-tune the initial detection results, and finally get the location and classification results of the lesions. Therefore, according to the characteristics of plant diseases and pests detection, common methods often improve on the backbone structure or its feature map, anchor ratio, ROIpooling and loss function. In 2017, Fuentes et al. [<a href="#CR59">59</a>] first used Faster R-CNN to locate tomato diseases and pests directly, combined with deep feature extractors such as VGG-Net and ResNet, the mAP value reached 85.98% in a dataset containing 5000 tomato diseases and pests of 9 categories. In 2019, Ozguven et al. [<a href="#CR60">60</a>] proposed a Faster R-CNN structure for automatic detection of beet leaf spot disease by changing the parameters of CNN model. 155 images were trained and tested. The results show that the overall correct classification rate of this method is 95.48%. Zhou et al. [<a href="#CR61">61</a>] presented a fast rice disease detection method based on the fusion of FCM-KM and Faster R-CNN. The application results of 3010 images showed that: the detection accuracy and time of rice blast, bacterial blight, and sheath blight are 96.71%/0.65Â s, 97.53%/0.82Â s and 98.26%/0.53Â s respectively. Xie et al. [<a href="#CR62">62</a>] proposed a Faster DR-IACNN model based on the self-built grape leaf disease dataset (GLDD) and Faster R-CNN detection algorithm, the Inception-v1 module, Inception-ResNet-v2 module and SE are introduced. The proposed model achieved higher feature extraction ability, the mAP accuracy was 81.1% and the detection speed was 15.01FPS. The two-stage detection network has been devoted to improving the detection speed to improve the real-time and practicability of the detection system, but compared with the single-stage detection network, it is still not concise enough, and the inference speed is still not fast enough.</p> </div> <div class="plantdiseasesandpestsdetectionbasedononestagenetwork" title="sec"> <div class="title" tagx="title" title="title">
Plant diseases and pests detection based on one stage network</div> <p id="Par30">The one-stage object detection algorithm has eliminated the region proposal stage, but directly adds the detection head to the backbone network for classification and regression, thus greatly improving the inference speed of the detection network. The single-stage detection network is divided into two types, SSD and YOLO, both of which use the whole image as the input of the network, and directly return the position of the bounding box and the category to which it belongs at the output layer.</p> <p id="Par31">Compared with the traditional convolutional neural network, the SSD selects VGG16 as the trunk of the network, and adds a feature pyramid network to obtain features from different layers and make predictions. Singh et al. [<a href="#CR63">63</a>] built the PlantDoc dataset for plant disease detection. Considering that the application should predict in mobile CPU in real time, an application based on MobileNets and SSD was established to simplify the detection of model parameters. Sun et al. [<a href="#CR64">64</a>] presented an instance detection method of multi-scale feature fusion based on convolutional neural network, which is improved on the basis of SSD to detect maize leaf blight under complex background. The proposed method combined data preprocessing, feature fusion, feature sharing, disease detection and other steps. The mAP of the new model is higher (from 71.80 to 91.83%) than that of the original SSD model. The FPS of the new model has also improved (from 24 to 28.4), reaching the standard of real-time detection.</p> <p id="Par32">YOLO considers the detection task as a regression problem, and uses global information to directly predict the bounding box and category of the object to achieve end-to-end detection of a single CNN network. YOLO can achieve global optimization and greatly improve the detection speed while satisfying higher accuracy. Prakruti et al. [<a href="#CR65">65</a>] presented a method to detect pests and diseases on images captured under uncontrolled conditions in tea gardens. YOLOv3 was used to detect pests and diseases. While ensuring real-time availability of the system, about 86% mAP was achieved with 50% IOU. Zhang et al. [<a href="#CR66">66</a>] combined the pooling of spatial pyramids with the improved YOLOv3, deconvolution is implemented by using the combination of up-sampling and convolution operation, which enables the algorithm to effectively detect small size crop pest samples in the image and reduces the problem of relatively low recognition accuracy due to the diversity of crop pest attitudes and scales. The average recognition accuracy can reach 88.07% by testing 20 class of pests collected in real scene.</p> <p id="Par33">In addition, there are many studies on using detection network to identify diseases and pests [<a href="#CR47">47</a>, <a href="#CR67">67</a>â€"<a href="#CR73">73</a>]. With the development of object detection network in computer vision, it is believed that more and more new detection models will be applied in plant diseases and pests detection in the future. In summary, in the field of plant diseases and pests detection which emphasizes detection accuracy at this stage, more models based on two-stage are used, and in the field of plant diseases and pests detection which pursue detection speed more models based on one-stage are used.</p> <p id="Par34">Can detection network replace classification network? The task of detection network is to solve the location problem of plant diseases and pests. The task of classification network is to judge the class of plant diseases and pests. Visually, the hidden information of detection network includes the category information, that is, the category information of plant diseases and pests that need to be located needs to be known beforehand, and the corresponding annotation information should be given in advance to judge the location of plant diseases and pests. From this point of view, the detection network seems to include the steps of the classification network, that is, the detection network can answer â€œwhat kind of plant diseases and pests are in what placeâ€�. But there is a misconception, in which â€œwhat kind of plant diseases and pestsâ€� is given a priori, that is, what is labelled during training is not necessarily the real result. In the case of strong model differentiation, that is, when the detection network can give accurate results, the detection network can answer â€œwhat kind of plant diseases and pests are in what placeâ€� to a certain extent. However, in the real world, in many cases, it cannot uniquely reflect the uniqueness of plant diseases and pests categories, only can answer â€œwhat kind of plant diseases and pests may be in what placeâ€�, then the involvement of the classification network is necessary. Thus, the detection network cannot replace the classification network.</p> </div> </div> <div class="segmentationnetwork" title="sec"> <div class="title" tagx="title" title="title">
Segmentation network</div> <p id="Par35">Segmentation network converts the plant diseases and pests detection task to semantic and even instance segmentation of lesions and normal areas. It not only finely divides the lesion area, but also obtains the location, category and corresponding geometric properties (including length, width, area, outline, center, etc.). It can be roughly divided into: Fully Convolutional Networks (FCN) [<a href="#CR74">74</a>] and Mask R-CNN [<a href="#CR75">75</a>].</p> <div class="fcn" title="sec"> <div class="title" tagx="title" title="title">
FCN</div> <p id="Par36">Full convolution neural network (FCN) is the basis of image semantics segmentation. At present, almost all semantics segmentation models are based on FCN. FCN first extracts and codes the features of the input image using convolution, then gradually restores the feature image to the size of the input image by deconvolution or up sampling. Based on the differences in FCN network structure, the plant diseases and pests segmentation methods can be divided into conventional FCN, U-net [<a href="#CR76">76</a>] and SegNet [<a href="#CR77">77</a>]. </p><ol> <li> <p id="Par37">Conventional FCN. Wang et al. [<a href="#CR78">78</a>] presented a new method of maize leaf disease segmentation based on full convolution neural network to solve the problem that traditional computer vision is susceptible to different illumination and complex background, and the segmentation accuracy reached 96.26. Wang et al. [<a href="#CR79">79</a>] proposed a plant diseases and pests segmentation method based on improved FCN. In this method, a convolution layer was used to extract multi-layer feature information from the input maize leaf lesion image, and the size and resolution of the input image were restored by deconvolution operation. Compared with the original FCN method, not only the integrity of the lesion was guaranteed, but also the segmentation of small lesion area was highlighted, and the accuracy rate reached 95.87%.</p> </li> <li> <p id="Par38">U-net. U-net is not only a classical FCN structure, but also a typical encoder-decoder structure. It is characterized by introducing a layer-hopping connection, fusing the feature map in the coding stage with that in the decoding stage, which is beneficial to the recovery of segmentation details. Lin et al. [<a href="#CR80">80</a>] used U-net based convolutional neural network to segment 50 cucumber powdery mildew leaves collected in natural environment. Compared with the original U-net, a batch normalization layer was added behind each convolution layer, making the neural network insensitive to weight initialization. The experiment shows that the convolutional neural network based on U-net can accurately segment powdery mildew on cucumber leaves at the pixel level with an average pixel accuracy of 96.08%, which is superior to the existing K-means, Random-forest and GBDT methods. The U-net method can segment the lesion area in a complex background, and still has good segmentation accuracy and segmentation speed with fewer samples.</p> </li> <li> <p id="Par39">SegNet. It is also a classical encoderâ€"decoder structure. Its feature is that the up-sampling operation in the decoder takes advantage of the index of the largest pooling operation in the encoder. Kerkech et al. [<a href="#CR81">81</a>] presented an image segmentation method for unmanned aerial vehicles. Visible and infrared images (480 samples from each range) were segmented using SegNet to identify four categories: shadows, ground, healthy and symptomatic grape vines. The detection rates of the proposed method on grape vines and leaves were 92% and 87%, respectively.</p> </li> </ol> <p /> </div> <div class="maskr-cnn" title="sec"> <div class="title" tagx="title" title="title">
Mask R-CNN</div> <p id="Par40">Mask R-CNN is one of the most commonly used image instance segmentation methods at present. It can be considered as a multitask learning method based on detection and segmentation network. When multiple lesions of the same type have adhesion or overlap, instance segmentation can separate individual lesions and further count the number of lesions. However, semantic segmentation often treats multiple lesions of the same type as a whole. Stewart et al. [<a href="#CR82">82</a>] trained a Mask R-CNN model to segment maize northern leaf blight (NLB) lesions in an unmanned aerial vehicle image. The trained model can accurately detect and segment a single lesion. At the IOU threshold of 0.50, the IOU between the baseline true value and the predicted lesion was 0.73, and the average accuracy was 0.96. Also, some studies combine the Mask R-CNN framework with object detection networks for plant diseases and pests detection. Wang et al. [<a href="#CR83">83</a>] used two different models, Faster R-CNN and ask R-CNN, in which Faster R-CNN was used to identify the class of tomato diseases and Mask R-CNN was used to detect and segment the location and shape of the infected area. The results showed that the proposed model can quickly and accurately identify 11 class of tomato diseases, and divide the location and shape of infected areas. Mask R-CNN reached a high detection rate of 99.64% for all class of tomato diseases.</p> <p id="Par41">Compared with the classification and detection network methods, the segmentation method has advantages in obtaining the lesion information. However, like the detection network, it requires a lot of annotation data, and its annotation information is pixel by pixel, which often takes a lot of effort and cost.</p> </div> </div> </div> <div class="datasetandperformancecomparison" title="sec"> <div class="title" tagx="title" title="title">
Dataset and performance comparison</div> <p id="Par42">This section first gives a brief introduction to the plant diseases and pests related datasets and the evaluation index of deep learning model, then compares and analyses the related models of plant diseases and pests detection based on deep learning in recent years.</p> <div class="datasetsforplantdiseasesandpestsdetection" title="sec"> <div class="title" tagx="title" title="title">
Datasets for plant diseases and pests detection</div> <p id="Par43">Plant diseases and pests detection datasets are the basis for research work. Compared with ImageNet, PASCAL-VOC2007/2012 and COCO in computer vision tasks, there is not a large and unified dataset for plant diseases and pests detection. The plant diseases and pests dataset can be acquired by self-collection, network collection and use of public datasets. Among them, self-collection of image dataset is often obtained by unmanned aerial remote sensing, ground camera photography, Internet of Things monitoring video or video recording, aerial photography of unmanned aerial vehicle with camera, hyperspectral imager, near-infrared spectrometer, and so on. Public datasets typically come from PlantVillage, an existing well-known public standard library. Relatively, self-collected datasets of plant diseases and pests in real natural environment are more practical. Although more and more researchers have opened up the images collected in the field, it is difficult to compare them uniformly based on different class of diseases under different detection objects and scenarios. This section provides links to a variety of plant diseases and pests detection datasets in conjunction with existing studies. As shown in Table <a href="#Tab4">4</a>. <div class="table-wrap_UNKNOWN" id="Tab4"><span class="label" tagx="label" title="label">Table 4</span> </div></p><p>Common datasets for plant diseases and pests detection</p>  <table frame="hsides" rules="groups"> <thead> <tr> <th align="left">Species</th> <th align="left">Collection environment</th> <th align="left">Link</th> </tr> </thead> <tbody> <tr> <td align="left">PlantVillage-Dataset: 50,000 images of classified plant diseases of 14 crop varieties and 26 diseases [<a href="#CR84">84</a>]</td> <td align="left">Detached leaves on a plain background</td> <td align="left"><a href="https://github.com/spMohanty/PlantVillage-Dataset">https://github.com/spMohanty/PlantVillage-Dataset</a></td> </tr> <tr> <td align="left">Rice Leaf Diseases Data Set: three classes of diseases: Bacterial leaf blight, Brown spot, and Leaf smut, each having 40 images [<a href="#CR85">85</a>, <a href="#CR86">86</a>]</td> <td align="left">Captured with a white background in direct sunlight</td> <td align="left"><a href="https://archive.ics.uci.edu/ml/datasets/Rice+Leaf+Diseases">https://archive.ics.uci.edu/ml/datasets/Rice+Leaf+Diseases</a></td> </tr> <tr> <td align="left">Image Database for Plant Disease Symptoms (PDDB): 2326 images of 171 diseases and other disorders affecting 21 plant species [<a href="#CR87">87</a>]</td> <td align="left">Field</td> <td align="left"><a href="https://www.digipathos-rep.cnptia.embrapa.br">https://www.digipathos-rep.cnptia.embrapa.br</a></td> </tr> <tr> <td align="left">New Plant Diseases Dataset (Augmented): 87Â K rgb images of healthy and diseased crop leaves which is categorized into 38 different classes</td> <td align="left">Detached leaves on a plain background</td> <td align="left"><a href="https://www.kaggle.com/vipoooool/new-plant-diseases-dataset/">https://www.kaggle.com/vipoooool/new-plant-diseases-dataset/</a></td> </tr> <tr> <td align="left">38 disease classes from PlantVillage dataset and 1 background class from Stanfordâ€™s open dataset of background imagesâ€"DAGS [<a href="#CR88">88</a>]</td> <td align="left">Network</td> <td align="left"><a href="https://github.com/MarkoArsenovic/DeepLearning_PlantDiseases">https://github.com/MarkoArsenovic/DeepLearning_PlantDiseases</a></td> </tr> <tr> <td align="left">18,222 images annotated with 105,705 northern leaf blight (NLB) lesions [<a href="#CR89">89</a>]</td> <td align="left">Field</td> <td align="left"><a href="https://osf.io/p67rz/">https://osf.io/p67rz/</a></td> </tr> <tr> <td align="left">40 classes of insects from rice, maize, soybean, sugarcane and cotton crops</td> <td align="left">Field</td> <td align="left"><a href="http://www.nbair.res.in/insectpests/pestsearch.php">http://www.nbair.res.in/insectpests/pestsearch.php</a></td> </tr> <tr> <td align="left">17,624 high quality JPG image data of rice, wheat and maize of 200Â GB</td> <td align="left">Field</td> <td align="left"><a href="http://www.icgroupcas.cn/website_bchtk/index.html">http://www.icgroupcas.cn/website_bchtk/index.html</a></td> </tr> <tr> <td align="left">PlantDoc dataset: 2598 data points in total across 13 plant species and up to 17 classes of diseases [<a href="#CR63">63</a>]</td> <td align="left">Field</td> <td align="left"> <p><a href="https://github.com/pratikkayal/PlantDoc-Object-Detection-Dataset">https://github.com/pratikkayal/PlantDoc-Object-Detection-Dataset</a></p> <p><a href="https://github.com/pratikkayal/PlantDoc-Dataset">https://github.com/pratikkayal/PlantDoc-Dataset</a></p> </td> </tr> <tr> <td align="left">Northern Leaf Blight (NLB) dataset for Maize</td> <td align="left">Field</td> <td align="left"><a href="https://bisque.cyverse.org/client_service/browser?resource=/data_service/dataset">https://bisque.cyverse.org/client_service/browser?resource=/data_service/dataset</a></td> </tr> <tr> <td align="left">3651 images of apple leaf disease [<a href="#CR90">90</a>]</td> <td align="left">Field</td> <td align="left"><a href="https://www.kaggle.com/c/plantpathology-2020-fgvc7">https://www.kaggle.com/c/plantpathology-2020-fgvc7</a></td> </tr> <tr> <td align="left">IP102: Insect Pest Recognition Database: 75,000 images belonging to 102 categories [<a href="#CR91">91</a>]</td> <td align="left">Field</td> <td align="left"><a href="https://github.com/xpwu95/IP102">https://github.com/xpwu95/IP102</a></td> </tr> <tr> <td align="left">A database of eight common tomato pest images [<a href="#CR92">92</a>]</td> <td align="left">Network</td> <td align="left"><a href="https://data.mendeley.com/datasets/s62zm6djd2/1">https://data.mendeley.com/datasets/s62zm6djd2/1</a></td> </tr> </tbody> </table>  <p /> </div> <div class="evaluationindices" title="sec"> <div class="title" tagx="title" title="title">
Evaluation indices</div> <p id="Par44">Evaluation indices can vary depending on the focus of the study. Common evaluation indices include <span class="inline-formula" title="inline-formula"> </span></p><div class="alternatives" title="alternatives"> <div class="tex" title="tex"> <div class="tex-math_UNKNOWN" id="M1">
\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$Precision$$\end{document}</div> </div><div class="math_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
Precision</div> </div></div><span class="inline-graphic" title="inline-graphic" /></div>, <span class="inline-formula" title="inline-formula"> <div class="alternatives" title="alternatives"> <div class="tex" title="tex"> <div class="tex-math_UNKNOWN" id="M3">
\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$Recall$$\end{document}</div> </div><div class="math_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
Recall</div> </div></div><span class="inline-graphic" title="inline-graphic" /></div></span>, mean Average Precision (mAP) and the harmonic Mean F1 score based on <span class="inline-formula" title="inline-formula"> <div class="alternatives" title="alternatives"> <div class="tex" title="tex"> <div class="tex-math_UNKNOWN" id="M5">
\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$Precision$$\end{document}</div> </div><div class="math_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
Precision</div> </div></div><span class="inline-graphic" title="inline-graphic" /></div></span> and <span class="inline-formula" title="inline-formula"> <div class="alternatives" title="alternatives"> <div class="tex" title="tex"> <div class="tex-math_UNKNOWN" id="M7">
\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$Recall$$\end{document}</div> </div><div class="math_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
Recall</div> </div></div><span class="inline-graphic" title="inline-graphic" /></div></span>.<p /> <p id="Par45"><span class="inline-formula" title="inline-formula"> </span></p><div class="alternatives" title="alternatives"> <div class="tex" title="tex"> <div class="tex-math_UNKNOWN" id="M9">
\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$Precision$$\end{document}</div> </div><div class="math_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
Precision</div> </div></div><span class="inline-graphic" title="inline-graphic" /></div> and <span class="inline-formula" title="inline-formula"> <div class="alternatives" title="alternatives"> <div class="tex" title="tex"> <div class="tex-math_UNKNOWN" id="M11">
\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$Recall$$\end{document}</div> </div><div class="math_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
Recall</div> </div></div><span class="inline-graphic" title="inline-graphic" /></div></span> are defined as: <div class="disp-formula" title="disp-formula"><span class="label" tagx="label" title="label">1</span><div class="alternatives" title="alternatives"> <div class="tex" title="tex"> <div class="tex-math_UNKNOWN" id="M13">
\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$Precision = \frac{TP}{{TP + FP}} \cdot 100\% ,$$\end{document}</div> </div><div class="math_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
P</div> <div class="mi_UNKNOWN">
r</div> <div class="mi_UNKNOWN">
e</div> <div class="mi_UNKNOWN">
c</div> <div class="mi_UNKNOWN">
i</div> <div class="mi_UNKNOWN">
s</div> <div class="mi_UNKNOWN">
i</div> <div class="mi_UNKNOWN">
o</div> <div class="mi_UNKNOWN">
n</div> <div class="mo_UNKNOWN">
=</div> <div class="mfrac_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
TP</div> </div> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
T</div> <div class="mi_UNKNOWN">
P</div> <div class="mo_UNKNOWN">
+</div> <div class="mi_UNKNOWN">
F</div> <div class="mi_UNKNOWN">
P</div> </div> </div> <div class="mo_UNKNOWN">
Â·</div> <div class="mn_UNKNOWN">
100</div> <div class="mo_UNKNOWN">
%</div> <div class="mo_UNKNOWN">
,</div> </div></div><div class="graphic" position="anchor" /> </div> </div> <div class="disp-formula" title="disp-formula"><span class="label" tagx="label" title="label">2</span><div class="alternatives" title="alternatives"> <div class="tex" title="tex"> <div class="tex-math_UNKNOWN" id="M15">
\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$Recall = \frac{TP}{{TP + FN}} \cdot 100\% .$$\end{document}</div> </div><div class="math_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
R</div> <div class="mi_UNKNOWN">
e</div> <div class="mi_UNKNOWN">
c</div> <div class="mi_UNKNOWN">
a</div> <div class="mi_UNKNOWN">
l</div> <div class="mi_UNKNOWN">
l</div> <div class="mo_UNKNOWN">
=</div> <div class="mfrac_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
TP</div> </div> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
T</div> <div class="mi_UNKNOWN">
P</div> <div class="mo_UNKNOWN">
+</div> <div class="mi_UNKNOWN">
F</div> <div class="mi_UNKNOWN">
N</div> </div> </div> <div class="mo_UNKNOWN">
Â·</div> <div class="mn_UNKNOWN">
100</div> <div class="mo_UNKNOWN">
%</div> <div class="mo_UNKNOWN">
.</div> </div></div><div class="graphic" position="anchor" /> </div> </div> <p /> <p id="Par46">In Formula (<a href="#Equ1">1</a>) and Formula (<a href="#Equ2">2</a>), TP (True Positive) is true-positive, predicted to be 1 and actually 1, indicating the number of lesions correctly identified by the algorithm. FP (False Positive) is false-positive, predicted to be 1 and actually 0, indicating the number of lesions incorrectly identified by the algorithm. FN (False Negative) is false-negative, predicted to be 0 and actually 1, indicating the number of unrecognized lesions.</p> <p id="Par47">Detection accuracy is usually assessed using mAP. The average accuracy of each category in the dataset needs to be calculated first: </p><div class="disp-formula" title="disp-formula"><span class="label" tagx="label" title="label">3</span><div class="alternatives" title="alternatives"> <div class="tex" title="tex"> <div class="tex-math_UNKNOWN" id="M17">
\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$P_{average} = \mathop \sum \limits_{j = 1}^{{N\left( {class} \right)}} Precision\left( j \right) \cdot Recall\left( j \right) \cdot 100\% .$$\end{document}</div> </div><div class="math_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="msub_UNKNOWN"> <div class="mi_UNKNOWN">
P</div> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
average</div> </div> </div> <div class="mo_UNKNOWN">
=</div> <div class="munderover_UNKNOWN"> <div class="mo_UNKNOWN">
âˆ‘</div> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
j</div> <div class="mo_UNKNOWN">
=</div> <div class="mn_UNKNOWN">
1</div> </div> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
N</div> <div class="mfenced_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
class</div> </div> </div> </div> </div> <div class="mi_UNKNOWN">
P</div> <div class="mi_UNKNOWN">
r</div> <div class="mi_UNKNOWN">
e</div> <div class="mi_UNKNOWN">
c</div> <div class="mi_UNKNOWN">
i</div> <div class="mi_UNKNOWN">
s</div> <div class="mi_UNKNOWN">
i</div> <div class="mi_UNKNOWN">
o</div> <div class="mi_UNKNOWN">
n</div> <div class="mfenced_UNKNOWN"> <div class="mi_UNKNOWN">
j</div> </div> <div class="mo_UNKNOWN">
Â·</div> <div class="mi_UNKNOWN">
R</div> <div class="mi_UNKNOWN">
e</div> <div class="mi_UNKNOWN">
c</div> <div class="mi_UNKNOWN">
a</div> <div class="mi_UNKNOWN">
l</div> <div class="mi_UNKNOWN">
l</div> <div class="mfenced_UNKNOWN"> <div class="mi_UNKNOWN">
j</div> </div> <div class="mo_UNKNOWN">
Â·</div> <div class="mn_UNKNOWN">
100</div> <div class="mo_UNKNOWN">
%</div> <div class="mo_UNKNOWN">
.</div> </div></div><div class="graphic" position="anchor" /> </div> </div> <p /> <p id="Par48">In the above-mentioned formula, <span class="inline-formula" title="inline-formula"> </span></p><div class="alternatives" title="alternatives"> <div class="tex" title="tex"> <div class="tex-math_UNKNOWN" id="M19">
\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$N\left( {class} \right)$$\end{document}</div> </div><div class="math_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
N</div> <div class="mfenced_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
class</div> </div> </div> </div></div><span class="inline-graphic" title="inline-graphic" /></div> represents the number of all categories, <span class="inline-formula" title="inline-formula"> <div class="alternatives" title="alternatives"> <div class="tex" title="tex"> <div class="tex-math_UNKNOWN" id="M21">
\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$Precision\left( j \right)$$\end{document}</div> </div><div class="math_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
P</div> <div class="mi_UNKNOWN">
r</div> <div class="mi_UNKNOWN">
e</div> <div class="mi_UNKNOWN">
c</div> <div class="mi_UNKNOWN">
i</div> <div class="mi_UNKNOWN">
s</div> <div class="mi_UNKNOWN">
i</div> <div class="mi_UNKNOWN">
o</div> <div class="mi_UNKNOWN">
n</div> <div class="mfenced_UNKNOWN"> <div class="mi_UNKNOWN">
j</div> </div> </div></div><span class="inline-graphic" title="inline-graphic" /></div></span> and <span class="inline-formula" title="inline-formula"> <div class="alternatives" title="alternatives"> <div class="tex" title="tex"> <div class="tex-math_UNKNOWN" id="M23">
\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$Recall\left( j \right)$$\end{document}</div> </div><div class="math_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
R</div> <div class="mi_UNKNOWN">
e</div> <div class="mi_UNKNOWN">
c</div> <div class="mi_UNKNOWN">
a</div> <div class="mi_UNKNOWN">
l</div> <div class="mi_UNKNOWN">
l</div> <div class="mfenced_UNKNOWN"> <div class="mi_UNKNOWN">
j</div> </div> </div></div><span class="inline-graphic" title="inline-graphic" /></div></span> represents the precision and recall of class <i>j</i> respectively.<p /> <p id="Par49">Average accuracy for each category is defined as mAP: </p><div class="disp-formula" title="disp-formula"><span class="label" tagx="label" title="label">4</span><div class="alternatives" title="alternatives"> <div class="tex" title="tex"> <div class="tex-math_UNKNOWN" id="M25">
\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$mAP = \frac{{P_{average} }}{{N\left( {class} \right)}}.$$\end{document}</div> </div><div class="math_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
m</div> <div class="mi_UNKNOWN">
A</div> <div class="mi_UNKNOWN">
P</div> <div class="mo_UNKNOWN">
=</div> <div class="mfrac_UNKNOWN"> <div class="msub_UNKNOWN"> <div class="mi_UNKNOWN">
P</div> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
average</div> </div> </div> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
N</div> <div class="mfenced_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
class</div> </div> </div> </div> </div> <div class="mo_UNKNOWN">
.</div> </div></div><div class="graphic" position="anchor" /> </div> </div> <p /> <p id="Par50">The greater the value of <span class="inline-formula" title="inline-formula"> </span></p><div class="alternatives" title="alternatives"> <div class="tex" title="tex"> <div class="tex-math_UNKNOWN" id="M27">
\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$mAP$$\end{document}</div> </div><div class="math_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
mAP</div> </div></div><span class="inline-graphic" title="inline-graphic" /></div>, the higher the recognition accuracy of the algorithm; conversely, the lower the accuracy of the algorithm.<p /> <p id="Par51">F1 score is also introduced to measure the accuracy of the model. F1 score takes into account both the accuracy and recall of the model. The formula is </p><div class="disp-formula" title="disp-formula"><span class="label" tagx="label" title="label">5</span><div class="alternatives" title="alternatives"> <div class="tex" title="tex"> <div class="tex-math_UNKNOWN" id="M29">
\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$${\text{F1}} = \frac{2 Precision \cdot Recall}{{Precision + Recall}} \cdot 100\% .$$\end{document}</div> </div><div class="math_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mtext_UNKNOWN">
F1</div> <div class="mo_UNKNOWN">
=</div> <div class="mfrac_UNKNOWN"> <div class="mrow_UNKNOWN"> <div class="mn_UNKNOWN">
2</div> <div class="mi_UNKNOWN">
P</div> <div class="mi_UNKNOWN">
r</div> <div class="mi_UNKNOWN">
e</div> <div class="mi_UNKNOWN">
c</div> <div class="mi_UNKNOWN">
i</div> <div class="mi_UNKNOWN">
s</div> <div class="mi_UNKNOWN">
i</div> <div class="mi_UNKNOWN">
o</div> <div class="mi_UNKNOWN">
n</div> <div class="mo_UNKNOWN">
Â·</div> <div class="mi_UNKNOWN">
R</div> <div class="mi_UNKNOWN">
e</div> <div class="mi_UNKNOWN">
c</div> <div class="mi_UNKNOWN">
a</div> <div class="mi_UNKNOWN">
l</div> <div class="mi_UNKNOWN">
l</div> </div> <div class="mrow_UNKNOWN"> <div class="mi_UNKNOWN">
P</div> <div class="mi_UNKNOWN">
r</div> <div class="mi_UNKNOWN">
e</div> <div class="mi_UNKNOWN">
c</div> <div class="mi_UNKNOWN">
i</div> <div class="mi_UNKNOWN">
s</div> <div class="mi_UNKNOWN">
i</div> <div class="mi_UNKNOWN">
o</div> <div class="mi_UNKNOWN">
n</div> <div class="mo_UNKNOWN">
+</div> <div class="mi_UNKNOWN">
R</div> <div class="mi_UNKNOWN">
e</div> <div class="mi_UNKNOWN">
c</div> <div class="mi_UNKNOWN">
a</div> <div class="mi_UNKNOWN">
l</div> <div class="mi_UNKNOWN">
l</div> </div> </div> <div class="mo_UNKNOWN">
Â·</div> <div class="mn_UNKNOWN">
100</div> <div class="mo_UNKNOWN">
%</div> <div class="mo_UNKNOWN">
.</div> </div></div><div class="graphic" position="anchor" /> </div> </div> <p /> <p id="Par52">Frames per second (FPS) is used to evaluate the recognition speed. The more frames per second, the faster the algorithm recognition speed; conversely, the slower the algorithm recognition speed.</p> </div> <div class="performancecomparisonofexistingalgorithms" title="sec"> <div class="title" tagx="title" title="title">
Performance comparison of existing algorithms</div> <p id="Par53">At present, the research on plant diseases and pests based on deep learning involves a wide range of crops, including all kinds of vegetables, fruits and food crops. The tasks completed include not only the basic tasks of classification, detection and segmentation, but also more complex tasks such as the judgment of infection degree.</p> <p id="Par54">At present, most of the current deep learning-based methods for plant diseases and pests detection are applied on specific datasets, many datasets are not publicly available, there is still no single publicly available and comprehensive dataset that will allow all algorithms to be uniformly compared. With the continuous development of deep learning, the application performance of some typical algorithms on different datasets has been gradually improved, and the mAP, F1 score and FPS of the algorithms have all been increased.</p> <p id="Par55">The breakthroughs achieved in the existing studies are amazing, but due to the fact that there is still a certain gap between the complexity of the infectious diseases and pests images in the existing studies and the real-time field diseases and pests detection based on mobile devices. Subsequent studies will need to find breakthroughs in larger, more complex, and more realistic datasets.</p> </div> </div> <div class="challenges" title="sec"> <div class="title" tagx="title" title="title">
Challenges</div> <div class="smalldatasetsizeproblem" title="sec"> <div class="title" tagx="title" title="title">
Small dataset size problem</div> <p id="Par56">At present, deep learning methods are widely used in various computer vision tasks, plant diseases and pests detection is generally regarded as specific application in the field of agriculture. There are too few agricultural plant diseases and pests samples available. Compared with open standard libraries, self-collected data sets are small in size and laborious in labeling data. Compared with more than 14Â million sample data in ImageNet datasets, the most critical problem facing plant diseases and pests detection is the problem of small samples. In practice, some plant diseases have low incidence and high cost of disease image acquisition, resulting in only a few or dozen training data collected, which limits the application of deep learning methods in the field of plant diseases and pests identification. In fact, for the problem of small samples, there are currently three different solutions.</p> <div class="dataamplification,synthesisandgeneration" title="sec"> <div class="title" tagx="title" title="title">
Data amplification, synthesis and generation</div> <p id="Par57">Data amplification is a key component of training deep learning models. An optimized data amplification strategy can effectively improve the plant diseases and pests detection effect. The most common method of plant diseases and pests image expansion is to acquire more samples using image processing operations such as mirroring, rotating, shifting, warping, filtering, contrast adjustment, and so on for the original plant diseases and pests samples. In addition, Generative Adversarial Networks (GANs) [<a href="#CR93">93</a>] and Variational automatic encoder (VAE) [<a href="#CR94">94</a>] can generate more diverse samples to enrich limited datasets.</p> </div> <div class="transferlearningandfine-tuningclassicalnetworkmodel" title="sec"> <div class="title" tagx="title" title="title">
Transfer learning and fine-tuning classical network model</div> <p id="Par58">Transfer learning (TL) transfers knowledge learned from generic large datasets to specialized areas with relatively small amounts of data. When transfer learning develops a model for newly collected unlabeled samples, it can start with a training model by a similar known dataset. After fine-tuning parameters or modifying components, it can be applied to localized plant disease and pest detection, which can reduce the cost of model training and enable the convolution neural network to adapt to small sample data. Oppenheim et al. [<a href="#CR95">95</a>] collected infected potato images of different sizes, hues and shapes under natural light and classified by fine-tuning the VGG network. The results showed that, the transfer learning and training of new networks were effective. Too et al. [<a href="#CR96">96</a>] evaluated various classical networks by fine-tuning and contrast. The experimental results showed that the accuracy of Dense-Nets improved with the number of iterations. Chen et al. [<a href="#CR97">97</a>] used transfer learning and fine-tuning to identify rice disease images under complex background conditions and achieved an average accuracy of 92.00%, which proves that the performance of transfer learning is better than training from scratch.</p> </div> <div class="reasonablenetworkstructuredesign" title="sec"> <div class="title" tagx="title" title="title">
Reasonable network structure design</div> <p id="Par59">By designing a reasonable network structure, the sample requirements can be greatly reduced. Zhang et al. [<a href="#CR98">98</a>] constructed a three-channel convolution neural network model for plant leaf disease recognition by combining three color components. Each channel TCCNN component is composed of three color RGB leaf disease images. Liu et al. [<a href="#CR99">99</a>] presented an improved CNN method for identifying grape leaf diseases. The model used a depth-separable convolution instead of a standard convolution to alleviate overfitting and reduce the number of parameters. For the different size of grape leaf lesions, the initial structure was applied to the model to improve the ability of multi-scale feature extraction. Compared with the standard ResNet and GoogLeNet structures, this model has faster convergence speed and higher accuracy during training. The recognition accuracy of this algorithm was 97.22%.</p> </div> </div> <div class="fine-grainedidentificationofsmall-sizelesionsinearlyidentification" title="sec"> <div class="title" tagx="title" title="title">
Fine-grained identification of small-size lesions in early identification</div> <div class="small-sizelesionsinearlyidentification" title="sec"> <div class="title" tagx="title" title="title">
Small-size lesions in early identification</div> <p id="Par60">Accurate early detection of plant diseases is essential to maximize the yield [<a href="#CR36">36</a>]. In the actual early identification of plant diseases and pests, due to the small size of the lesion object itself, multiple down sampling processes in the deep feature extraction network tend to cause small-scale objects to be ignored. Moreover, due to the background noise problem on the collected images, large-scale complex background may lead to more false detection, especially on low-resolution images. In view of the shortage of existing algorithms, the improvement direction of small object detection algorithm is analyzed, and several strategies such as attention mechanism are proposed to improve the performance of small target detection.</p> <p id="Par61">The use of attention mechanism makes resources allocated more rationally. The essence of attention mechanism is to quickly find region of interest and ignore unimportant information. By learning the characteristics of plant diseases and pests images, features can be separated using weighted sum method with weighted coefficient, and the background noise in the image can be suppressed. Specifically, the attention mechanism module can get a salient image, and seclude the object from the background, and the Softmax function can be used to manipulate the feature image, and combine it with the original feature image to obtain new fusion features for noise reduction purposes. In future studies on early recognition of plant diseases and pests, attention mechanisms can be used to effectively select information and allocate more resources to region of interest to achieve more accurate detection. Karthik et al. [<a href="#CR100">100</a>] applied attention mechanism on the residual network and experiments were carried out using the plantVillage dataset, which achieved 98% overall accuracy.</p> </div> <div class="fine-grainedidentification" title="sec"> <div class="title" tagx="title" title="title">
Fine-grained identification</div> <p id="Par62">First, there is a large difference within the class, that is, the visual characteristics of plant diseases and pests belonging to the same class are quite different. The reason is that the aforementioned external factors such as uneven illumination, dense occlusion, blurred equipment dithering and other interferences, resulting in different image samples belonging to the same kind of diseases and pests differ greatly. Plant diseases and pests detection in complex scenarios is a very challenging task of fine-grained recognition [<a href="#CR101">101</a>]. The existence of growth variations of diseases and pests results in distinct differences in the characterization of the same diseases and pests at different stages, forming the â€œintra-class differenceâ€� fine-grained characteristics.</p> <p id="Par63">Secondly, there is fuzziness between classes, that is, objects of different classes have some similarity. There are many detailed classifications of biological subspecies and subclasses of different kinds of diseases and pests, and there are some similarities of biological morphology and life habits among the subclasses, which lead to the problem of fine-grained identification of â€œinter-class similarityâ€�. Barbedo believed that similar symptoms could be produced, which even phytopathologists could not correctly distinguish [<a href="#CR102">102</a>].</p> <p id="Par64">Thirdly, background disturbance makes it impossible for plant diseases and pests to appear in a very clean background in the real world. Background can be very complex and interfere with objects of interest, which makes plant diseases and pests detection more difficult. Some literature often ignores this issue because images are captured under controlled conditions [<a href="#CR103">103</a>].</p> <p id="Par65">Relying on the existing deep learning methods can not effectively identify the fine-grained characteristics of diseases and pests that exist naturally in the application of the above actual agricultural scenarios, resulting in technical difficulties such as low identification accuracy and generalization robustness, which has long restricted the performance improvement of decision-making management of diseases and pests by the Intelligent Agricultural Internet of Things [<a href="#CR104">104</a>]. The existing research is only suitable for fine-grained identification of fewer class of diseases and pests, can not solve the problem of large-scale, large-category, accurate and efficient identification of diseases and pests, and is difficult to deploy directly to the mobile terminals of smart agriculture.</p> </div> </div> <div class="detectionperformanceundertheinfluenceofilluminationandocclusion" title="sec"> <div class="title" tagx="title" title="title">
Detection performance under the influence of illumination and occlusion</div> <div class="lightingproblems" title="sec"> <div class="title" tagx="title" title="title">
Lighting problems</div> <p id="Par66">Previous studies have collected images of plant diseases and pests mostly in indoor light boxes [<a href="#CR105">105</a>]. Although this method can effectively eliminate the influence of external light to simplify image processing, it is quite different from the images collected under real natural light. Because natural light changes very dynamically, and the range in which the camera can accept dynamic light sources is limited, it is easy to cause image color distortion when above or below this limit. In addition, due to the difference of view angle and distance during image collection, the apparent characteristics of plant diseases and pests change greatly, which brings great difficulties to the visual recognition algorithm.</p> </div> <div class="occlusionproblem" title="sec"> <div class="title" tagx="title" title="title">
Occlusion problem</div> <p id="Par67">At present, most researchers intentionally avoid the recognition of plant diseases and pests in complex environments. They only focus on a single background. They use the method of directly intercepting the area of interest to the collected images, but seldom consider the occlusion problem. As a result, the recognition accuracy under occlusion is low and the practicability is greatly reduced. Occlusion problems are common in real natural environments, including blade occlusion caused by changes in blade posture, branch occlusion, light occlusion caused by external lighting, and mixed occlusion caused by different types of occlusion. The difficulties of plant diseases and pests identification under occlusion are the lack of features and noise overlap caused by occlusion. Different occlusion conditions have different degrees of impact on the recognition algorithm, resulting in false detection or even missed detection. In recent years, with the maturity of deep learning algorithms under restricted conditions, some researchers have gradually challenged the identification of plant diseases and pests under occluded conditions [<a href="#CR106">106</a>, <a href="#CR107">107</a>], and significant progress has been made, which lays a good foundation for the application of plant diseases and pests identification in real-world scenarios. However, occlusion is random and complex. The training of the basic framework is difficult and the dependence on the performance of hardware devices still exists, we should strengthen the innovation and optimization of the basic framework, including the design of lightweight network architecture. The exploration of GAN and other aspects should be enhanced, while ensuring the accuracy of detection, the difficulty of model training should be reduced. GAN has prominent advantages in dealing with posture changes and chaotic background, but its design is not yet mature, and it is easy to crash in learning and cause model uncontrollable problems during training. We should strengthen the exploration of network performance to make it easier to quantify the quality of the model.</p> </div> <div class="detectionspeedproblem" title="sec"> <div class="title" tagx="title" title="title">
Detection speed problem</div> <p id="Par68">Compared with traditional methods, deep learning algorithms have better results, but their computational complexity is also higher. If the detection accuracy is guaranteed, the model needs to fully learn the characteristics of the image and increase the computational load, which will inevitably lead to slow detection speed and can not meet the needs of real-time. In order to ensure the detection speed, it is usually necessary to reduce the amount of calculation. However, this will cause insufficient training and result in false or missed detection. Therefore, it is important to design an efficient algorithm with both detection accuracy and detection speed.</p> <p id="Par69">Plant diseases and pests detection methods based on deep learning include three main links in agricultural applications: data labeling, model training and model inference. In real-time agricultural applications, more attention is paid to model inference. Currently, most plant diseases and pests detection methods focus on the accuracy of recognition. Little attention is paid to the efficiency of model inference. In reference [<a href="#CR108">108</a>], to improve the efficiency of the model calculation process to meet the actual agricultural needs, a deep separable convolution structure model for plant leaf disease detection was introduced. Several models were trained and tested. The classification accuracy of Reduced MobileNet was 98.34%, the parameters were 29 times less than VGG, and 6 times less than MobileNet. This shows an effective compromise between delay and accuracy, which is suitable for real-time crop diseases diagnosis on resource-constrained mobile devices.</p> </div> </div> </div> <div class="conclusionsandfuturedirections" title="sec"> <div class="title" tagx="title" title="title">
Conclusions and future directions</div> <p id="Par70">Compared with traditional image processing methods, which deal with plant diseases and pests detection tasks in several steps and links, plant diseases and pests detection methods based on deep learning unify them into end-to-end feature extraction, which has a broad development prospects and great potential. Although plant diseases and pests detection technology is developing rapidly, it has been moving from academic research to agricultural application, there is still a certain distance from the mature application in the real natural environment, and there are still some problems to be solved.</p> <div class="plantdiseasesandpestsdetectiondataset" title="sec"> <div class="title" tagx="title" title="title">
Plant diseases and pests detection dataset</div> <p id="Par71">Deep learning technology has made some achievements in the identification of plant diseases and pests. Various image recognition algorithms have also been further developed and extended, which provides a theoretical basis for the identification of specific diseases and pests. However, the collection of image samples in previous studies mostly come from the characterization of disease spots, insect appearance characteristics or the characterization of insect pests and leaves. Most of the research results are limited to the laboratory environment and are applicable only to the plant diseases and pests images obtained at the time. The main reason for this is that the growth of plants is cyclical, continuous, seasonal and regional. Similarly, the characteristics of the same disease or pest at different growing stages of crops are different. Images of different plant species vary from region to region. As a result, most of the existing research results are not universal. Even with a high recognition rate in a single trial, the validity of the data obtained at other times cannot be guaranteed.</p> <p id="Par72">Most of the existing studies are based on the images generated in the visible range, but the electromagnetic wave outside the visible range also contains a lot of information, so the comprehensive information such as visible light, near infrared, multi-spectral should be fused to achieve the acquisition of plant diseases and pests dataset. Future research should focus on multi-information fusion method to obtain and identify plant diseases and pests information.</p> <p id="Par73">In addition, image databases of different kinds of plant diseases and pests in real natural environments are still in the blank stage. Future research should make full use of the data information acquisition platform such as portable field spore auto-capture instrument, unmanned aerial vehicle aerial photography system, agricultural internet of things monitoring equipment, which performs large-area and coverage identification of farmland and makes up for the lack of randomness of image samples in previous studies. Also, it can ensures the comprehensiveness and accuracy of dataset, and improves the generality of the algorithm.</p> </div> <div class="earlyrecognitionofplantdiseasesandpests" title="sec"> <div class="title" tagx="title" title="title">
Early recognition of plant diseases and pests</div> <p id="Par74">In the application of plant diseases and pests identification, the manifestation symptoms are not obvious, so early diagnosis is very difficult whether it is by visual observation or computer interpretation. However, the research significance and demand of early diagnosis are greater, which is more conducive to the prevention and control of plant diseases and pests and prevent their spread and development. The best image quality can be obtained when the sunlight is sufficient, and taking pictures in cloudy weather will increase the complexity of image preprocessing and reduce the recognition effect. In addition, in the early stage of plant diseases and pests occurrence, even high-resolution images are difficult to analyze. It is necessary to combine meteorological and plant protection data such as temperature and humidity to realize the recognition and prediction of diseases and pests. By consulting the existing research literatures, there are few reports on the early diagnosis of plant diseases and pests.</p> </div> <div class="networktrainingandlearning" title="sec"> <div class="title" tagx="title" title="title">
Network training and learning</div> <p id="Par75">When plant diseases and pests are visually identified manually, it is difficult to collect samples of all plant diseases and pests types, and many times only healthy data (positive samples) are available. However, most of the current plant diseases and pests detection methods based on deep learning are supervised learning based on a large number of diseases and pests samples, so manual collection of labelled datasets requires a lot of manpower, so unsupervised learning needs to be explored. Deep learning is a black box, which requires a large number of labelled training samples for end-to-end learning and has poor interpretability. Therefore, how to use the prior knowledge of brain-inspired computing and human-like visual cognitive model to guide the training and learning of the network is also a direction worthy of studying. At the same time, deep models need a large amount of memory and are extremely time-consuming during testing, which makes them unsuitable for deployment on mobile platforms with limited resources. It is important to study how to reduce complexity and obtain fast-executing models without losing accuracy. Finally, the selection of appropriate hyper-parameters has always been a major obstacle to the application of deep learning model to new tasks, such as learning rate, filter size, step size and number, these hyper-parameters have a strong internal dependence, any small adjustment may have a greater impact on the final training results.</p> </div> <div class="interdisciplinaryresearch" title="sec"> <div class="title" tagx="title" title="title">
Interdisciplinary research</div> <p id="Par76">Only by more closely integrating empirical data with theories such as agronomic plant protection, can we establish a field diagnosis model that is more in line with the rules of crop growth, and will further improve the effectiveness and accuracy of plant diseases and pests identification. In the future, it is necessary to go from image analysis at the surface level to identification of the occurrence mechanism of diseases and pests, and transition from simple experimental environment to practical application research that comprehensively considers crop growth law, environmental factors, etc.</p> <p id="Par77">In summary, with the development of artificial intelligence technology, the research focus of plant diseases and pests detection based on machine vision has shifted from classical image processing and machine learning methods to deep learning methods, which solved the difficult problems that could not be solved by traditional methods. There is still a long distance from the popularization of practical production and application, but this technology has great development potential and application value. To fully explore the potential of this technology, the joint efforts of experts from relevant disciplines are needed to effectively integrate the experience knowledge of agriculture and plant protection with deep learning algorithms and models, so as to make plant diseases and pests detection based on deep learning mature. Also, the research results should be integrated into agricultural machinery equipment to truly land the corresponding theoretical results.</p> </div> </div> </div> <div class="back" title="back"> <div class="fn-group" title="fn-group"> <div class="fn-type-" title=""> <p><b>Publisher's Note</b></p> <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p> </div> </div> <div class="ack" title="ack"> <div class="title" tagx="title" title="title">
Acknowledgements</div> <p>Appreciations are given to the editors and reviewer of the Journal Plant Method.</p> </div> <div class="title" tagx="title" title="title">
Authorsâ€™ contributions</div> <p>JL designed the research. JL and XW conducted the experiments and data analysis and wrote the manuscript. XW revised the manuscript. Both authors read and approved the final manuscript.</p> <div class="title" tagx="title" title="title">
Funding</div> <p>This study was supported by the Facility Horticulture Laboratory of Universities in Shandong with Project Numbers 2019YY003, 2018YY016, 2018YY043 and 2018YY044; school level High-level Talents Project 2018RC002; Youth Fund Project of Philosophy and Social Sciences of Weifang College of Science and Technology with project numbers 2018WKRQZ008 and 2018WKRQZ008-3; Key research and development plan of Shandong Province with Project Number 2019RKA07012, 2019GNC106034Â andÂ 2020RKA07036; Research and Development Plan of Applied Technology in Shouguang with Project Number 2018JH12; 2018 innovation fund of Science and Technology Development centre of the China Ministry of Education with Project Number 2018A02013; 2019 basic capacity construction project of private colleges and universities in Shandong Province; and Weifang Science and Technology Development Programme with project numbers 2019GX081 and 2019GX082,Â Special project of Ideological and political education of Weifang University of science and technologyÂ (W19SZ70Z01).</p> <div class="title" tagx="title" title="title">
Availability of data and materials</div> <p>For relevant data and codes, please contact the corresponding author of this manuscript.</p> <div class="title" tagx="title" title="title">
Ethics approval and consent to participate</div> <p id="Par78">Not applicable.</p> <div class="title" tagx="title" title="title">
Consent for publication</div> <p id="Par79">Not applicable.</p> <div class="title" tagx="title" title="title">
Competing interests</div> <p id="Par80">The authors declare that they have no competing interests.</p> <div class="references">
References</div> <div tag="ref-list"> <ul> <div class="title" tagx="title" title="title">
References</div> <li tag="ref"><a name="CR1" /><span class="label" tagx="label" title="label">1.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Lee</span><span class="given-names" tagx="given-names" title="given-names">SH</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chan</span><span class="given-names" tagx="given-names" title="given-names">CS</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Mayo</span><span class="given-names" tagx="given-names" title="given-names">SJ</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Remagnino</span><span class="given-names" tagx="given-names" title="given-names">P</span></span></span><span class="mixed-article-title" title="mixed-article-title">How deep learning extracts and learns leaf features for plant classification</span><span class="source" tagx="source" title="source">Pattern Recogn</span><span class="year" tagx="year" title="year">2017</span><span class="volume" tagx="volume" title="volume">71</span><span class="fpage" tagx="fpage" title="fpage">1</span><span class="lpage" tagx="lpage" title="lpage">13</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.patcog.2017.05.015">10.1016/j.patcog.2017.05.015</a></span></span></li> <li tag="ref"><a name="CR2" /><span class="label" tagx="label" title="label">2.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Tsaftaris</span><span class="given-names" tagx="given-names" title="given-names">SA</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Minervini</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Scharr</span><span class="given-names" tagx="given-names" title="given-names">H</span></span></span><span class="mixed-article-title" title="mixed-article-title">Machine learning for plant phenotyping needs image processing</span><span class="source" tagx="source" title="source">Trends Plant Sci</span><span class="year" tagx="year" title="year">2016</span><span class="volume" tagx="volume" title="volume">21</span><span class="issue" tagx="issue" title="issue">12</span><span class="fpage" tagx="fpage" title="fpage">989</span><span class="lpage" tagx="lpage" title="lpage">991</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.tplants.2016.10.002">10.1016/j.tplants.2016.10.002</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/27810146">27810146</a></span></span></li> <li tag="ref"><a name="CR3" /><span class="label" tagx="label" title="label">3.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Fuentes</span><span class="given-names" tagx="given-names" title="given-names">A</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Yoon</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Park</span><span class="given-names" tagx="given-names" title="given-names">DS</span></span></span><span class="mixed-article-title" title="mixed-article-title">Deep learning-based techniques for plant diseases recognition in real-field scenarios</span><span class="source" tagx="source" title="source">Advanced concepts for intelligent vision systems</span><span class="year" tagx="year" title="year">2020</span><span class="publisher-loc" tagx="publisher-loc" title="publisher-loc">Cham</span><span class="publisher-name" tagx="publisher-name" title="publisher-name">Springer</span></span></li> <li tag="ref"><a name="CR4" /><span class="label" tagx="label" title="label">4.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Yang</span><span class="given-names" tagx="given-names" title="given-names">D</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Li</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Peng</span><span class="given-names" tagx="given-names" title="given-names">Z</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wang</span><span class="given-names" tagx="given-names" title="given-names">P</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wang</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Yang</span><span class="given-names" tagx="given-names" title="given-names">H</span></span></span><span class="mixed-article-title" title="mixed-article-title">MF-CNN: traffic flow prediction using convolutional neural network and multi-features fusion</span><span class="source" tagx="source" title="source">IEICE Trans Inf Syst</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">102</span><span class="issue" tagx="issue" title="issue">8</span><span class="fpage" tagx="fpage" title="fpage">1526</span><span class="lpage" tagx="lpage" title="lpage">1536</span><span class="pub-id"><a href="https://dx.doi.org/10.1587/transinf.2018EDP7330">10.1587/transinf.2018EDP7330</a></span></span></li> <li tag="ref"><a name="CR5" /><span class="label" tagx="label" title="label">5.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Sundararajan</span><span class="given-names" tagx="given-names" title="given-names">SK</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Sankaragomathi</span><span class="given-names" tagx="given-names" title="given-names">B</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Priya</span><span class="given-names" tagx="given-names" title="given-names">DS</span></span></span><span class="mixed-article-title" title="mixed-article-title">Deep belief cnn feature representation based content based image retrieval for medical images</span><span class="source" tagx="source" title="source">J Med Syst</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">43</span><span class="issue" tagx="issue" title="issue">6</span><span class="fpage" tagx="fpage" title="fpage">1</span><span class="lpage" tagx="lpage" title="lpage">9</span><span class="pub-id"><a href="https://dx.doi.org/10.1007/s10916-019-1305-6">10.1007/s10916-019-1305-6</a></span></span></li> <li tag="ref"><a name="CR6" /><span class="label" tagx="label" title="label">6.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Melnyk</span><span class="given-names" tagx="given-names" title="given-names">P</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">You</span><span class="given-names" tagx="given-names" title="given-names">Z</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Li</span><span class="given-names" tagx="given-names" title="given-names">K</span></span></span><span class="mixed-article-title" title="mixed-article-title">A high-performance CNN method for offline handwritten chinese character recognition and visualization</span><span class="source" tagx="source" title="source">Soft Comput</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">24</span><span class="fpage" tagx="fpage" title="fpage">7977</span><span class="lpage" tagx="lpage" title="lpage">7987</span><span class="pub-id"><a href="https://dx.doi.org/10.1007/s00500-019-04083-3">10.1007/s00500-019-04083-3</a></span></span></li> <li tag="ref"><a name="CR7" /><span class="label" tagx="label" title="label">7.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Li</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Mi</span><span class="given-names" tagx="given-names" title="given-names">Y</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Li</span><span class="given-names" tagx="given-names" title="given-names">G</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Ju</span><span class="given-names" tagx="given-names" title="given-names">Z</span></span></span><span class="mixed-article-title" title="mixed-article-title">CNN-based facial expression recognition from annotated rgb-d images for humanâ€"robot interaction</span><span class="source" tagx="source" title="source">Int J Humanoid Robot</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">16</span><span class="issue" tagx="issue" title="issue">04</span><span class="fpage" tagx="fpage" title="fpage">504</span><span class="lpage" tagx="lpage" title="lpage">505</span><span class="pub-id"><a href="https://dx.doi.org/10.1142/S0219843619410020">10.1142/S0219843619410020</a></span></span></li> <li tag="ref"><a name="CR8" /><span class="label" tagx="label" title="label">8.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Kumar</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Singh</span><span class="given-names" tagx="given-names" title="given-names">SK</span></span></span><span class="mixed-article-title" title="mixed-article-title">Occluded thermal face recognition using bag of CNN(BoCNN)</span><span class="source" tagx="source" title="source">IEEE Signal Process Lett</span><span class="year" tagx="year" title="year">2020</span><span class="volume" tagx="volume" title="volume">27</span><span class="fpage" tagx="fpage" title="fpage">975</span><span class="lpage" tagx="lpage" title="lpage">979</span><span class="pub-id"><a href="https://dx.doi.org/10.1109/LSP.2020.2996429">10.1109/LSP.2020.2996429</a></span></span></li> <li tag="ref"><a name="CR9" /><span class="label" tagx="label" title="label">9.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wang</span><span class="given-names" tagx="given-names" title="given-names">X</span></span></span><span class="mixed-article-title" title="mixed-article-title">Deep learning in object recognition, detection, and segmentation</span><span class="source" tagx="source" title="source">Found Trends Signal Process</span><span class="year" tagx="year" title="year">2016</span><span class="volume" tagx="volume" title="volume">8</span><span class="issue" tagx="issue" title="issue">4</span><span class="fpage" tagx="fpage" title="fpage">217</span><span class="lpage" tagx="lpage" title="lpage">382</span><span class="pub-id"><a href="https://dx.doi.org/10.1561/2000000071">10.1561/2000000071</a></span></span></li> <li tag="ref"><a name="CR10" /><span class="label" tagx="label" title="label">10.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Boulent</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Foucher</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">ThÃ©au</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">St-Charles</span><span class="given-names" tagx="given-names" title="given-names">PL</span></span></span><span class="mixed-article-title" title="mixed-article-title">Convolutional neural networks for the automatic identification of plant diseases</span><span class="source" tagx="source" title="source">Front Plant Sci</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">10</span><span class="fpage" tagx="fpage" title="fpage">941</span><span class="pub-id"><a href="https://dx.doi.org/10.3389/fpls.2019.00941">10.3389/fpls.2019.00941</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/31396250">31396250</a></span></span></li> <li tag="ref"><a name="CR11" /><span class="label" tagx="label" title="label">11.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Kumar</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Kaur</span><span class="given-names" tagx="given-names" title="given-names">R</span></span></span><span class="mixed-article-title" title="mixed-article-title">Plant disease detection using image processingâ€"a review</span><span class="source" tagx="source" title="source">Int J Comput Appl</span><span class="year" tagx="year" title="year">2015</span><span class="volume" tagx="volume" title="volume">124</span><span class="issue" tagx="issue" title="issue">2</span><span class="fpage" tagx="fpage" title="fpage">6</span><span class="lpage" tagx="lpage" title="lpage">9</span></span></li> <li tag="ref"><a name="CR12" /><span class="label" tagx="label" title="label">12.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Martineau</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Conte</span><span class="given-names" tagx="given-names" title="given-names">D</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Raveaux</span><span class="given-names" tagx="given-names" title="given-names">R</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Arnault</span><span class="given-names" tagx="given-names" title="given-names">I</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Munier</span><span class="given-names" tagx="given-names" title="given-names">D</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Venturini</span><span class="given-names" tagx="given-names" title="given-names">G</span></span></span><span class="mixed-article-title" title="mixed-article-title">A survey on image-based insect classification</span><span class="source" tagx="source" title="source">Pattern Recogn</span><span class="year" tagx="year" title="year">2016</span><span class="volume" tagx="volume" title="volume">65</span><span class="fpage" tagx="fpage" title="fpage">273</span><span class="lpage" tagx="lpage" title="lpage">284</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.patcog.2016.12.020">10.1016/j.patcog.2016.12.020</a></span></span></li> <li tag="ref"><a name="CR13" /><span class="label" tagx="label" title="label">13.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Jayme</span><span class="given-names" tagx="given-names" title="given-names">GAB</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Luciano</span><span class="given-names" tagx="given-names" title="given-names">VK</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Bernardo</span><span class="given-names" tagx="given-names" title="given-names">HV</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Rodrigo</span><span class="given-names" tagx="given-names" title="given-names">VC</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Katia</span><span class="given-names" tagx="given-names" title="given-names">LN</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Claudia</span><span class="given-names" tagx="given-names" title="given-names">VG</span></span><span class="etal" title="etal"><i>et al.</i></span></span><span class="mixed-article-title" title="mixed-article-title">Annotated plant pathology databases for image-based detection and recognition of diseases</span><span class="source" tagx="source" title="source">IEEE Latin Am Trans</span><span class="year" tagx="year" title="year">2018</span><span class="volume" tagx="volume" title="volume">16</span><span class="issue" tagx="issue" title="issue">6</span><span class="fpage" tagx="fpage" title="fpage">1749</span><span class="lpage" tagx="lpage" title="lpage">1757</span><span class="pub-id"><a href="https://dx.doi.org/10.1109/TLA.2018.8444395">10.1109/TLA.2018.8444395</a></span></span></li> <li tag="ref"><a name="CR14" /><span class="label" tagx="label" title="label">14.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Kaur</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Pandey</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Goel</span><span class="given-names" tagx="given-names" title="given-names">S</span></span></span><span class="mixed-article-title" title="mixed-article-title">Plants disease identification and classification through leaf images: a survey</span><span class="source" tagx="source" title="source">Arch Comput Methods Eng</span><span class="year" tagx="year" title="year">2018</span><span class="volume" tagx="volume" title="volume">26</span><span class="issue" tagx="issue" title="issue">4</span><span class="fpage" tagx="fpage" title="fpage">1</span><span class="lpage" tagx="lpage" title="lpage">24</span></span></li> <li tag="ref"><a name="CR15" /><span class="label" tagx="label" title="label">15.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Shekhawat</span><span class="given-names" tagx="given-names" title="given-names">RS</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Sinha</span><span class="given-names" tagx="given-names" title="given-names">A</span></span></span><span class="mixed-article-title" title="mixed-article-title">Review of image processing approaches for detecting plant diseases</span><span class="source" tagx="source" title="source">IET Image Process</span><span class="year" tagx="year" title="year">2020</span><span class="volume" tagx="volume" title="volume">14</span><span class="issue" tagx="issue" title="issue">8</span><span class="fpage" tagx="fpage" title="fpage">1427</span><span class="lpage" tagx="lpage" title="lpage">1439</span><span class="pub-id"><a href="https://dx.doi.org/10.1049/iet-ipr.2018.6210">10.1049/iet-ipr.2018.6210</a></span></span></li> <li tag="ref"><a name="CR16" /><span class="label" tagx="label" title="label">16.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Hinton</span><span class="given-names" tagx="given-names" title="given-names">GE</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Salakhutdinov</span><span class="given-names" tagx="given-names" title="given-names">R</span></span></span><span class="mixed-article-title" title="mixed-article-title">Reducing the dimensionality of data with neural networks</span><span class="source" tagx="source" title="source">Science</span><span class="year" tagx="year" title="year">2006</span><span class="volume" tagx="volume" title="volume">313</span><span class="issue" tagx="issue" title="issue">5786</span><span class="fpage" tagx="fpage" title="fpage">504</span><span class="lpage" tagx="lpage" title="lpage">507</span><span class="pub-id"><a href="https://dx.doi.org/10.1126/science.1127647">10.1126/science.1127647</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/16873662">16873662</a></span></span></li> <li tag="ref"><a name="CR17" /><span class="label" tagx="label" title="label">17.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Liu</span><span class="given-names" tagx="given-names" title="given-names">W</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wang</span><span class="given-names" tagx="given-names" title="given-names">Z</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Liu</span><span class="given-names" tagx="given-names" title="given-names">X</span></span><span class="etal" title="etal"><i>et al.</i></span></span><span class="mixed-article-title" title="mixed-article-title">A survey of deep neural network architectures and their applications</span><span class="source" tagx="source" title="source">Neurocomputing</span><span class="year" tagx="year" title="year">2017</span><span class="volume" tagx="volume" title="volume">234</span><span class="fpage" tagx="fpage" title="fpage">11</span><span class="lpage" tagx="lpage" title="lpage">26</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.neucom.2016.12.038">10.1016/j.neucom.2016.12.038</a></span></span></li> <li tag="ref"><a name="CR18" /><span class="label" tagx="label" title="label">18.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Fergus R. Deep learning methods for vision. CVPR 2012 Tutorial; 2012.</span></li> <li tag="ref"><a name="CR19" /><span class="label" tagx="label" title="label">19.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Bengio</span><span class="given-names" tagx="given-names" title="given-names">Y</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Courville</span><span class="given-names" tagx="given-names" title="given-names">A</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Vincent</span><span class="given-names" tagx="given-names" title="given-names">P</span></span></span><span class="mixed-article-title" title="mixed-article-title">Representation learning: a review and new perspectives</span><span class="source" tagx="source" title="source">IEEE Trans Pattern Anal Mach Intell</span><span class="year" tagx="year" title="year">2013</span><span class="volume" tagx="volume" title="volume">35</span><span class="issue" tagx="issue" title="issue">8</span><span class="fpage" tagx="fpage" title="fpage">1798</span><span class="lpage" tagx="lpage" title="lpage">1828</span><span class="pub-id"><a href="https://dx.doi.org/10.1109/TPAMI.2013.50">10.1109/TPAMI.2013.50</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/23787338">23787338</a></span></span></li> <li tag="ref"><a name="CR20" /><span class="label" tagx="label" title="label">20.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Boureau YL, Le Roux N, Bach F, Ponce J, Lecun Y. [IEEE 2011 IEEE international conference on computer vision (ICCV)â€"Barcelona, Spain (2011.11.6â€"2011.11.13)] 2011 international conference on computer visionâ€"ask the locals: multi-way local pooling for image recognition; 2011. p. 2651â€"8.</span></li> <li tag="ref"><a name="CR21" /><span class="label" tagx="label" title="label">21.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Zeiler MD, Fergus R. Stochastic pooling for regularization of deep convolutional neural networks. Eprint Arxiv. <a href="http://arxiv.org/abs/1301.3557">arXiv:1301.3557</a>. 2013.</span></li> <li tag="ref"><a name="CR22" /><span class="label" tagx="label" title="label">22.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">TensorFlow. <a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a>.</span></li> <li tag="ref"><a name="CR23" /><span class="label" tagx="label" title="label">23.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Torch/PyTorch. <a href="https://pytorch.org/">https://pytorch.org/</a>.</span></li> <li tag="ref"><a name="CR24" /><span class="label" tagx="label" title="label">24.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Caffe. <a href="http://caffe.berkeleyvision.org/">http://caffe.berkeleyvision.org/</a>.</span></li> <li tag="ref"><a name="CR25" /><span class="label" tagx="label" title="label">25.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Theano. <a href="http://deeplearning.net/software/theano/">http://deeplearning.net/software/theano/</a>.</span></li> <li tag="ref"><a name="CR26" /><span class="label" tagx="label" title="label">26.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Krizhenvshky A, Sutskever I, Hinton G. Imagenet classification with deep convolutional networks. In: Proceedings of the conference neural information processing systems (NIPS), Lake Tahoe, NV, USA, 3â€"8 December; 2012. p. 1097â€"105.</span></li> <li tag="ref"><a name="CR27" /><span class="label" tagx="label" title="label">27.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V, Rabinovich A. Going deeper with convolutions. In: Proceedings of the 2015 IEEE conference on computer vision and pattern recognition, Boston, MA, USA, 7â€"12 June; 2015. p. 1â€"9.</span></li> <li tag="ref"><a name="CR28" /><span class="label" tagx="label" title="label">28.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. arXiv. <a href="http://arxiv.org/abs/1409.1556">arXiv:1409.1556</a>. 2014.</span></li> <li tag="ref"><a name="CR29" /><span class="label" tagx="label" title="label">29.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Xie S, Girshick R, DollÃ¡r P, Tu Z, He K. Aggregated residual transformations for deep neural networks. arXiv. <a href="http://arxiv.org/abs/1611.05431">arXiv:1611.05431</a>. 2017.</span></li> <li tag="ref"><a name="CR30" /><span class="label" tagx="label" title="label">30.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Szegedy C, Ioffe S, Vanhoucke V, et al. Inception-v4, inception-resnet and the impact of residual connections on learning. In: Proceedings of the AAAI conference on artificial intelligence. 2016.</span></li> <li tag="ref"><a name="CR31" /><span class="label" tagx="label" title="label">31.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Huang G, Lrj Z, Maaten LVD, et al. Densely connected convolutional networks. In: IEEE conference on computer vision and pattern recognition. 2017. p. 2261â€"9.</span></li> <li tag="ref"><a name="CR32" /><span class="label" tagx="label" title="label">32.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Howard AG, Zhu M, Chen B, Kalenichenko D, Wang W, Weyand T, Andreetto M, Adam H. MobileNets: efficient convolutional neural networks for mobile vision applications. arXiv. <a href="http://arxiv.org/abs/1704.04861">arXiv:1704.04861</a>. 2017.</span></li> <li tag="ref"><a name="CR33" /><span class="label" tagx="label" title="label">33.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Iandola FN, Han S, Moskewicz MW, Ashraf K, Dally WJ, Keutzer K. SqueezeNet: AlexNet-level accuracy with 50 Ã— fewer parameters and &amp;lt; 0.5 MB model size. arXiv. <a href="http://arxiv.org/abs/1602.07360">arXiv:1602.07360</a>. 2016.</span></li> <li tag="ref"><a name="CR34" /><span class="label" tagx="label" title="label">34.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Priyadharshini</span><span class="given-names" tagx="given-names" title="given-names">RA</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Arivazhagan</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Arun</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Mirnalini</span><span class="given-names" tagx="given-names" title="given-names">A</span></span></span><span class="mixed-article-title" title="mixed-article-title">Maize leaf disease classification using deep convolutional neural networks</span><span class="source" tagx="source" title="source">Neural Comput Appl</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">31</span><span class="issue" tagx="issue" title="issue">12</span><span class="fpage" tagx="fpage" title="fpage">8887</span><span class="lpage" tagx="lpage" title="lpage">8895</span><span class="pub-id"><a href="https://dx.doi.org/10.1007/s00521-019-04228-3">10.1007/s00521-019-04228-3</a></span></span></li> <li tag="ref"><a name="CR35" /><span class="label" tagx="label" title="label">35.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wen</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Shi</span><span class="given-names" tagx="given-names" title="given-names">Y</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Zhou</span><span class="given-names" tagx="given-names" title="given-names">X</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Xue</span><span class="given-names" tagx="given-names" title="given-names">Y</span></span></span><span class="mixed-article-title" title="mixed-article-title">Crop disease classification on inadequate low-resolution target images</span><span class="source" tagx="source" title="source">Sensors</span><span class="year" tagx="year" title="year">2020</span><span class="volume" tagx="volume" title="volume">20</span><span class="issue" tagx="issue" title="issue">16</span><span class="fpage" tagx="fpage" title="fpage">4601</span><span class="pub-id"><a href="https://dx.doi.org/10.3390/s20164601">10.3390/s20164601</a></span></span></li> <li tag="ref"><a name="CR36" /><span class="label" tagx="label" title="label">36.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Thangaraj</span><span class="given-names" tagx="given-names" title="given-names">R</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Anandamurugan</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Kaliappan</span><span class="given-names" tagx="given-names" title="given-names">VK</span></span></span><span class="mixed-article-title" title="mixed-article-title">Automated tomato leaf disease classification using transfer learning-based deep convolution neural network</span><span class="source" tagx="source" title="source">J Plant Dis Prot</span><span class="year" tagx="year" title="year">2020</span><span class="pub-id"><a href="https://dx.doi.org/10.1007/s41348-020-00403-0">10.1007/s41348-020-00403-0</a></span></span></li> <li tag="ref"><a name="CR37" /><span class="label" tagx="label" title="label">37.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Atila</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Uar</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Akyol</span><span class="given-names" tagx="given-names" title="given-names">K</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Uar</span><span class="given-names" tagx="given-names" title="given-names">E</span></span></span><span class="mixed-article-title" title="mixed-article-title">Plant leaf disease classification using efficientnet deep learning model</span><span class="source" tagx="source" title="source">Ecol Inform</span><span class="year" tagx="year" title="year">2021</span><span class="volume" tagx="volume" title="volume">61</span><span class="fpage" tagx="fpage" title="fpage">101182</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.ecoinf.2020.101182">10.1016/j.ecoinf.2020.101182</a></span></span></li> <li tag="ref"><a name="CR38" /><span class="label" tagx="label" title="label">38.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Sabrol</span><span class="given-names" tagx="given-names" title="given-names">H</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Kumar</span><span class="given-names" tagx="given-names" title="given-names">S</span></span></span><span class="mixed-article-title" title="mixed-article-title">Recent studies of image and soft computing techniques for plant disease recognition and classification</span><span class="source" tagx="source" title="source">Int J Comput Appl</span><span class="year" tagx="year" title="year">2015</span><span class="volume" tagx="volume" title="volume">126</span><span class="issue" tagx="issue" title="issue">1</span><span class="fpage" tagx="fpage" title="fpage">44</span><span class="lpage" tagx="lpage" title="lpage">55</span></span></li> <li tag="ref"><a name="CR39" /><span class="label" tagx="label" title="label">39.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Yalcin H, Razavi S. Plant classification using convolutional neural networks. In: 2016 5th international conference on agro-geoinformatics (agro-geoinformatics). New York: IEEE; 2016.</span></li> <li tag="ref"><a name="CR40" /><span class="label" tagx="label" title="label">40.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Fuentes A, Lee J, Lee Y, Yoon S, Park DS. Anomaly detection of plant diseases and insects using convolutional neural networks. In: ELSEVIER conference ISEM 2017â€"The International Society for Ecological Modelling Global Conference, 2017. 2017.</span></li> <li tag="ref"><a name="CR41" /><span class="label" tagx="label" title="label">41.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Hasan MJ, Mahbub S, Alom MS, Nasim MA. Rice disease identification and classification by integrating support vector machine with deep convolutional neural network. In: 2019 1st international conference on advances in science, engineering and robotics technology (ICASERT). 2019.</span></li> <li tag="ref"><a name="CR42" /><span class="label" tagx="label" title="label">42.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Thenmozhi</span><span class="given-names" tagx="given-names" title="given-names">K</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Reddy</span><span class="given-names" tagx="given-names" title="given-names">US</span></span></span><span class="mixed-article-title" title="mixed-article-title">Crop pest classification based on deep convolutional neural network and transfer learning</span><span class="source" tagx="source" title="source">Comput Electron Agric</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">164</span><span class="fpage" tagx="fpage" title="fpage">104906</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.compag.2019.104906">10.1016/j.compag.2019.104906</a></span></span></li> <li tag="ref"><a name="CR43" /><span class="label" tagx="label" title="label">43.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Fang</span><span class="given-names" tagx="given-names" title="given-names">T</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chen</span><span class="given-names" tagx="given-names" title="given-names">P</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Zhang</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wang</span><span class="given-names" tagx="given-names" title="given-names">B</span></span></span><span class="mixed-article-title" title="mixed-article-title">Crop leaf disease grade identification based on an improved convolutional neural network</span><span class="source" tagx="source" title="source">J Electron Imaging</span><span class="year" tagx="year" title="year">2020</span><span class="volume" tagx="volume" title="volume">29</span><span class="issue" tagx="issue" title="issue">1</span><span class="fpage" tagx="fpage" title="fpage">1</span><span class="pub-id"><a href="https://dx.doi.org/10.1117/1.JEI.29.1.013004">10.1117/1.JEI.29.1.013004</a></span></span></li> <li tag="ref"><a name="CR44" /><span class="label" tagx="label" title="label">44.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Nagasubramanian</span><span class="given-names" tagx="given-names" title="given-names">K</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Jones</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Singh</span><span class="given-names" tagx="given-names" title="given-names">AK</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Sarkar</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Singh</span><span class="given-names" tagx="given-names" title="given-names">A</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Ganapathysubramanian</span><span class="given-names" tagx="given-names" title="given-names">B</span></span></span><span class="mixed-article-title" title="mixed-article-title">Plant disease identification using explainable 3D deep learning on hyperspectral images</span><span class="source" tagx="source" title="source">Plant Methods</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">15</span><span class="issue" tagx="issue" title="issue">1</span><span class="fpage" tagx="fpage" title="fpage">1</span><span class="lpage" tagx="lpage" title="lpage">10</span><span class="pub-id"><a href="https://dx.doi.org/10.1186/s13007-019-0479-8">10.1186/s13007-019-0479-8</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/30622623">30622623</a></span></span></li> <li tag="ref"><a name="CR45" /><span class="label" tagx="label" title="label">45.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Picon</span><span class="given-names" tagx="given-names" title="given-names">A</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Seitz</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Alvarez-Gila</span><span class="given-names" tagx="given-names" title="given-names">A</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Mohnke</span><span class="given-names" tagx="given-names" title="given-names">P</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Echazarra</span><span class="given-names" tagx="given-names" title="given-names">J</span></span></span><span class="mixed-article-title" title="mixed-article-title">Crop conditional convolutional neural networks for massive multi-crop plant disease classification over cell phone acquired images taken on real field conditions</span><span class="source" tagx="source" title="source">Comput Electron Agric</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">167</span><span class="fpage" tagx="fpage" title="fpage">105093</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.compag.2019.105093">10.1016/j.compag.2019.105093</a></span></span></li> <li tag="ref"><a name="CR46" /><span class="label" tagx="label" title="label">46.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Tianjiao</span><span class="given-names" tagx="given-names" title="given-names">C</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wei</span><span class="given-names" tagx="given-names" title="given-names">D</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Juan</span><span class="given-names" tagx="given-names" title="given-names">Z</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chengjun</span><span class="given-names" tagx="given-names" title="given-names">X</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Rujing</span><span class="given-names" tagx="given-names" title="given-names">W</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wancai</span><span class="given-names" tagx="given-names" title="given-names">L</span></span><span class="etal" title="etal"><i>et al.</i></span></span><span class="mixed-article-title" title="mixed-article-title">Intelligent identification system of disease and insect pests based on deep learning</span><span class="source" tagx="source" title="source">China Plant Prot Guide</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">039</span><span class="issue" tagx="issue" title="issue">004</span><span class="fpage" tagx="fpage" title="fpage">26</span><span class="lpage" tagx="lpage" title="lpage">34</span></span></li> <li tag="ref"><a name="CR47" /><span class="label" tagx="label" title="label">47.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Dechant</span><span class="given-names" tagx="given-names" title="given-names">C</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wiesner-Hanks</span><span class="given-names" tagx="given-names" title="given-names">T</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chen</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Stewart</span><span class="given-names" tagx="given-names" title="given-names">EL</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Yosinski</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Gore</span><span class="given-names" tagx="given-names" title="given-names">MA</span></span><span class="etal" title="etal"><i>et al.</i></span></span><span class="mixed-article-title" title="mixed-article-title">Automated identification of northern leaf blight-infected maize plants from field imagery using deep learning</span><span class="source" tagx="source" title="source">Phytopathology</span><span class="year" tagx="year" title="year">2017</span><span class="volume" tagx="volume" title="volume">107</span><span class="fpage" tagx="fpage" title="fpage">1426</span><span class="lpage" tagx="lpage" title="lpage">1432</span><span class="pub-id"><a href="https://dx.doi.org/10.1094/PHYTO-11-16-0417-R">10.1094/PHYTO-11-16-0417-R</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/28653579">28653579</a></span></span></li> <li tag="ref"><a name="CR48" /><span class="label" tagx="label" title="label">48.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wiesner-Hanks</span><span class="given-names" tagx="given-names" title="given-names">T</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wu</span><span class="given-names" tagx="given-names" title="given-names">H</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Stewart</span><span class="given-names" tagx="given-names" title="given-names">E</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Dechant</span><span class="given-names" tagx="given-names" title="given-names">C</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Nelson</span><span class="given-names" tagx="given-names" title="given-names">RJ</span></span></span><span class="mixed-article-title" title="mixed-article-title">Millimeter-level plant disease detection from aerial photographs via deep learning and crowdsourced data</span><span class="source" tagx="source" title="source">Front Plant Sci</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">10</span><span class="fpage" tagx="fpage" title="fpage">1550</span><span class="pub-id"><a href="https://dx.doi.org/10.3389/fpls.2019.01550">10.3389/fpls.2019.01550</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/31921228">31921228</a></span></span></li> <li tag="ref"><a name="CR49" /><span class="label" tagx="label" title="label">49.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Shougang</span><span class="given-names" tagx="given-names" title="given-names">R</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Fuwei</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Xingjian</span><span class="given-names" tagx="given-names" title="given-names">G</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Peishen</span><span class="given-names" tagx="given-names" title="given-names">Y</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wei</span><span class="given-names" tagx="given-names" title="given-names">X</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Huanliang</span><span class="given-names" tagx="given-names" title="given-names">X</span></span></span><span class="mixed-article-title" title="mixed-article-title">Deconvolution-guided tomato leaf disease identification and lesion segmentation model</span><span class="source" tagx="source" title="source">J Agric Eng</span><span class="year" tagx="year" title="year">2020</span><span class="volume" tagx="volume" title="volume">36</span><span class="issue" tagx="issue" title="issue">12</span><span class="fpage" tagx="fpage" title="fpage">186</span><span class="lpage" tagx="lpage" title="lpage">195</span></span></li> <li tag="ref"><a name="CR50" /><span class="label" tagx="label" title="label">50.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Fujita E, Kawasaki Y, Uga H, Kagiwada S, Iyatomi H. Basic investigation on a robust and practical plant diagnostic system. In: IEEE international conference on machine learning &amp;amp; applications. New York: IEEE; 2016.</span></li> <li tag="ref"><a name="CR51" /><span class="label" tagx="label" title="label">51.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Mohanty</span><span class="given-names" tagx="given-names" title="given-names">SP</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Hughes</span><span class="given-names" tagx="given-names" title="given-names">DP</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">SalathÃ©</span><span class="given-names" tagx="given-names" title="given-names">M</span></span></span><span class="mixed-article-title" title="mixed-article-title">Using deep learning for image-based plant disease detection</span><span class="source" tagx="source" title="source">Front Plant Sci</span><span class="year" tagx="year" title="year">2016</span><span class="volume" tagx="volume" title="volume">7</span><span class="fpage" tagx="fpage" title="fpage">1419</span><span class="pub-id"><a href="https://dx.doi.org/10.3389/fpls.2016.01419">10.3389/fpls.2016.01419</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/27713752">27713752</a></span></span></li> <li tag="ref"><a name="CR52" /><span class="label" tagx="label" title="label">52.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Brahimi</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Arsenovic</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Laraba</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Sladojevic</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Boukhalfa</span><span class="given-names" tagx="given-names" title="given-names">K</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Moussaoui</span><span class="given-names" tagx="given-names" title="given-names">A</span></span></span><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Zhou</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chen</span><span class="given-names" tagx="given-names" title="given-names">F</span></span></span><span class="mixed-article-title" title="mixed-article-title">Deep learning for plant diseases: detection and saliency map visualisation</span><span class="source" tagx="source" title="source">Human and machine learning</span><span class="year" tagx="year" title="year">2018</span><span class="publisher-loc" tagx="publisher-loc" title="publisher-loc">Cham</span><span class="publisher-name" tagx="publisher-name" title="publisher-name">Springer International Publishing</span><span class="fpage" tagx="fpage" title="fpage">93</span><span class="lpage" tagx="lpage" title="lpage">117</span></span></li> <li tag="ref"><a name="CR53" /><span class="label" tagx="label" title="label">53.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Barbedo</span><span class="given-names" tagx="given-names" title="given-names">JG</span></span></span><span class="mixed-article-title" title="mixed-article-title">Plant disease identification from individual lesions and spots using deep learning</span><span class="source" tagx="source" title="source">Biosyst Eng</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">180</span><span class="fpage" tagx="fpage" title="fpage">96</span><span class="lpage" tagx="lpage" title="lpage">107</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.biosystemseng.2019.02.002">10.1016/j.biosystemseng.2019.02.002</a></span></span></li> <li tag="ref"><a name="CR54" /><span class="label" tagx="label" title="label">54.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Ren</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">He</span><span class="given-names" tagx="given-names" title="given-names">K</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Girshick</span><span class="given-names" tagx="given-names" title="given-names">R</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Sun</span><span class="given-names" tagx="given-names" title="given-names">J</span></span></span><span class="mixed-article-title" title="mixed-article-title">Faster R-CNN: towards real-time object detection with region proposal networks</span><span class="source" tagx="source" title="source">IEEE Trans Pattern Anal Mach Intell</span><span class="year" tagx="year" title="year">2017</span><span class="volume" tagx="volume" title="volume">39</span><span class="issue" tagx="issue" title="issue">6</span><span class="fpage" tagx="fpage" title="fpage">1137</span><span class="lpage" tagx="lpage" title="lpage">1149</span><span class="pub-id"><a href="https://dx.doi.org/10.1109/TPAMI.2016.2577031">10.1109/TPAMI.2016.2577031</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/27295650">27295650</a></span></span></li> <li tag="ref"><a name="CR55" /><span class="label" tagx="label" title="label">55.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Liu W, Anguelov D, Erhan D, Szegedy C, Berg AC. SSD: Single shot MultiBox detector. In: European conference on computer vision. Cham: Springer International Publishing; 2016.</span></li> <li tag="ref"><a name="CR56" /><span class="label" tagx="label" title="label">56.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Redmon J, Divvala S, Girshick R, Farhadi A. You only look once: unified, real-time object detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.</span></li> <li tag="ref"><a name="CR57" /><span class="label" tagx="label" title="label">57.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Redmon J, Farhadi A. Yolo9000: better, faster, stronger. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2017. p. 6517â€"25.</span></li> <li tag="ref"><a name="CR58" /><span class="label" tagx="label" title="label">58.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Redmon J, Farhadi A. Yolov3: an incremental improvement. arXiv preprint. <a href="http://arxiv.org/abs/1804.02767">arXiv:1804.02767</a>. 2018.</span></li> <li tag="ref"><a name="CR59" /><span class="label" tagx="label" title="label">59.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Fuentes</span><span class="given-names" tagx="given-names" title="given-names">A</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Yoon</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Kim</span><span class="given-names" tagx="given-names" title="given-names">SC</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Park</span><span class="given-names" tagx="given-names" title="given-names">DS</span></span></span><span class="mixed-article-title" title="mixed-article-title">A robust deep-learning-based detector for real-time tomato plant diseases and pests detection</span><span class="source" tagx="source" title="source">Sensors</span><span class="year" tagx="year" title="year">2017</span><span class="volume" tagx="volume" title="volume">17</span><span class="issue" tagx="issue" title="issue">9</span><span class="fpage" tagx="fpage" title="fpage">2022</span><span class="pub-id"><a href="https://dx.doi.org/10.3390/s17092022">10.3390/s17092022</a></span></span></li> <li tag="ref"><a name="CR60" /><span class="label" tagx="label" title="label">60.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Ozguven</span><span class="given-names" tagx="given-names" title="given-names">MM</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Adem</span><span class="given-names" tagx="given-names" title="given-names">K</span></span></span><span class="mixed-article-title" title="mixed-article-title">Automatic detection and classification of leaf spot disease in sugar beet using deep learning algorithms</span><span class="source" tagx="source" title="source">Phys A Statal Mech Appl</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">535</span><span class="issue" tagx="issue" title="issue">2019</span><span class="fpage" tagx="fpage" title="fpage">122537</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.physa.2019.122537">10.1016/j.physa.2019.122537</a></span></span></li> <li tag="ref"><a name="CR61" /><span class="label" tagx="label" title="label">61.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Zhou</span><span class="given-names" tagx="given-names" title="given-names">G</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Zhang</span><span class="given-names" tagx="given-names" title="given-names">W</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chen</span><span class="given-names" tagx="given-names" title="given-names">A</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">He</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Ma</span><span class="given-names" tagx="given-names" title="given-names">X</span></span></span><span class="mixed-article-title" title="mixed-article-title">Rapid detection of rice disease based on FCM-KM and faster R-CNN fusion</span><span class="source" tagx="source" title="source">IEEE Access</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">7</span><span class="fpage" tagx="fpage" title="fpage">143190</span><span class="lpage" tagx="lpage" title="lpage">143206</span><span class="pub-id"><a href="https://dx.doi.org/10.1109/ACCESS.2019.2943454">10.1109/ACCESS.2019.2943454</a></span></span></li> <li tag="ref"><a name="CR62" /><span class="label" tagx="label" title="label">62.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Xie</span><span class="given-names" tagx="given-names" title="given-names">X</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Ma</span><span class="given-names" tagx="given-names" title="given-names">Y</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Liu</span><span class="given-names" tagx="given-names" title="given-names">B</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">He</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wang</span><span class="given-names" tagx="given-names" title="given-names">H</span></span></span><span class="mixed-article-title" title="mixed-article-title">A deep-learning-based real-time detector for grape leaf diseases using improved convolutional neural networks</span><span class="source" tagx="source" title="source">Front Plant Sci</span><span class="year" tagx="year" title="year">2020</span><span class="volume" tagx="volume" title="volume">11</span><span class="fpage" tagx="fpage" title="fpage">751</span><span class="pub-id"><a href="https://dx.doi.org/10.3389/fpls.2020.00751">10.3389/fpls.2020.00751</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/32582266">32582266</a></span></span></li> <li tag="ref"><a name="CR63" /><span class="label" tagx="label" title="label">63.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Singh D, Jain N, Jain P, Kayal P, Kumawat S, Batra N. Plantdoc: a dataset for visual plant disease detection. In: Proceedings of the 7th ACM IKDD CoDS and 25th COMAD. 2019.</span></li> <li tag="ref"><a name="CR64" /><span class="label" tagx="label" title="label">64.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Sun</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Yang</span><span class="given-names" tagx="given-names" title="given-names">Y</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">He</span><span class="given-names" tagx="given-names" title="given-names">X</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wu</span><span class="given-names" tagx="given-names" title="given-names">X</span></span></span><span class="mixed-article-title" title="mixed-article-title">Northern maize leaf blight detection under complex field environment based on deep learning</span><span class="source" tagx="source" title="source">IEEE Access</span><span class="year" tagx="year" title="year">2020</span><span class="volume" tagx="volume" title="volume">8</span><span class="fpage" tagx="fpage" title="fpage">33679</span><span class="lpage" tagx="lpage" title="lpage">33688</span><span class="pub-id"><a href="https://dx.doi.org/10.1109/ACCESS.2020.2973658">10.1109/ACCESS.2020.2973658</a></span></span></li> <li tag="ref"><a name="CR65" /><span class="label" tagx="label" title="label">65.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Bhatt PV, Sarangi S, Pappula S. Detection of diseases and pests on images captured in uncontrolled conditions from tea plantations. In: Proc. SPIE 11008, autonomous air and ground sensing systems for agricultural optimization and phenotyping IV; 2019. p. 1100808. 10.1117/12.2518868.</span></li> <li tag="ref"><a name="CR66" /><span class="label" tagx="label" title="label">66.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Zhang</span><span class="given-names" tagx="given-names" title="given-names">B</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Zhang</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chen</span><span class="given-names" tagx="given-names" title="given-names">Y</span></span></span><span class="mixed-article-title" title="mixed-article-title">Crop pest identification based on spatial pyramid pooling and deep convolution neural network</span><span class="source" tagx="source" title="source">Trans Chin Soc Agric Eng</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">35</span><span class="issue" tagx="issue" title="issue">19</span><span class="fpage" tagx="fpage" title="fpage">209</span><span class="lpage" tagx="lpage" title="lpage">215</span></span></li> <li tag="ref"><a name="CR67" /><span class="label" tagx="label" title="label">67.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Ramcharan</span><span class="given-names" tagx="given-names" title="given-names">A</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">McCloskey</span><span class="given-names" tagx="given-names" title="given-names">P</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Baranowski</span><span class="given-names" tagx="given-names" title="given-names">K</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Mbilinyi</span><span class="given-names" tagx="given-names" title="given-names">N</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Mrisho</span><span class="given-names" tagx="given-names" title="given-names">L</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Ndalahwa</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Legg</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Hughes</span><span class="given-names" tagx="given-names" title="given-names">D</span></span></span><span class="mixed-article-title" title="mixed-article-title">A mobile-based deep learning model for cassava disease diagnosis</span><span class="source" tagx="source" title="source">Front Plant Sci</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">10</span><span class="fpage" tagx="fpage" title="fpage">272</span><span class="pub-id"><a href="https://dx.doi.org/10.3389/fpls.2019.00272">10.3389/fpls.2019.00272</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/30949185">30949185</a></span></span></li> <li tag="ref"><a name="CR68" /><span class="label" tagx="label" title="label">68.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Selvaraj</span><span class="given-names" tagx="given-names" title="given-names">G</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Vergara</span><span class="given-names" tagx="given-names" title="given-names">A</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Ruiz</span><span class="given-names" tagx="given-names" title="given-names">H</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Safari</span><span class="given-names" tagx="given-names" title="given-names">N</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Elayabalan</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Ocimati</span><span class="given-names" tagx="given-names" title="given-names">W</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Blomme</span><span class="given-names" tagx="given-names" title="given-names">G</span></span></span><span class="mixed-article-title" title="mixed-article-title">AI-powered banana diseases and pest detection</span><span class="source" tagx="source" title="source">Plant Methods</span><span class="year" tagx="year" title="year">2019</span><span class="pub-id"><a href="https://dx.doi.org/10.1186/s13007-019-0475-z">10.1186/s13007-019-0475-z</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/31728153">31728153</a></span></span></li> <li tag="ref"><a name="CR69" /><span class="label" tagx="label" title="label">69.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Tian</span><span class="given-names" tagx="given-names" title="given-names">Y</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Yang</span><span class="given-names" tagx="given-names" title="given-names">G</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wang</span><span class="given-names" tagx="given-names" title="given-names">Z</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Li</span><span class="given-names" tagx="given-names" title="given-names">E</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Liang</span><span class="given-names" tagx="given-names" title="given-names">Z</span></span></span><span class="mixed-article-title" title="mixed-article-title">Detection of apple lesions in orchards based on deep learning methods of CycleGAN and YOLOV3-dense</span><span class="source" tagx="source" title="source">J Sens</span><span class="year" tagx="year" title="year">2019</span><span class="pub-id"><a href="https://dx.doi.org/10.1155/2019/7630926">10.1155/2019/7630926</a></span></span></li> <li tag="ref"><a name="CR70" /><span class="label" tagx="label" title="label">70.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Zheng</span><span class="given-names" tagx="given-names" title="given-names">Y</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Kong</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Jin</span><span class="given-names" tagx="given-names" title="given-names">X</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wang</span><span class="given-names" tagx="given-names" title="given-names">X</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Zuo</span><span class="given-names" tagx="given-names" title="given-names">M</span></span></span><span class="mixed-article-title" title="mixed-article-title">CropDeep: the crop vision dataset for deep-learning-based classification and detection in precision agriculture</span><span class="source" tagx="source" title="source">Sensors</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">19</span><span class="fpage" tagx="fpage" title="fpage">1058</span><span class="pub-id"><a href="https://dx.doi.org/10.3390/s19051058">10.3390/s19051058</a></span></span></li> <li tag="ref"><a name="CR71" /><span class="label" tagx="label" title="label">71.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Arsenovic</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Karanovic</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Sladojevic</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Anderla</span><span class="given-names" tagx="given-names" title="given-names">A</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">StefanoviÄ‡</span><span class="given-names" tagx="given-names" title="given-names">D</span></span></span><span class="mixed-article-title" title="mixed-article-title">Solving current limitations of deep learning based approaches for plant disease detection</span><span class="source" tagx="source" title="source">Symmetry</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">11</span><span class="fpage" tagx="fpage" title="fpage">21</span><span class="pub-id"><a href="https://dx.doi.org/10.3390/sym11070939">10.3390/sym11070939</a></span></span></li> <li tag="ref"><a name="CR72" /><span class="label" tagx="label" title="label">72.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Fuentes</span><span class="given-names" tagx="given-names" title="given-names">AF</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Yoon</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Lee</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Park</span><span class="given-names" tagx="given-names" title="given-names">DS</span></span></span><span class="mixed-article-title" title="mixed-article-title">High-performance deep neural network-based tomato plant diseases and pests diagnosis system with refinement filter bank</span><span class="source" tagx="source" title="source">Front Plant Sci</span><span class="year" tagx="year" title="year">2018</span><span class="volume" tagx="volume" title="volume">9</span><span class="fpage" tagx="fpage" title="fpage">1162</span><span class="pub-id"><a href="https://dx.doi.org/10.3389/fpls.2018.01162">10.3389/fpls.2018.01162</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/30210509">30210509</a></span></span></li> <li tag="ref"><a name="CR73" /><span class="label" tagx="label" title="label">73.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Jiang</span><span class="given-names" tagx="given-names" title="given-names">P</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chen</span><span class="given-names" tagx="given-names" title="given-names">Y</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Liu</span><span class="given-names" tagx="given-names" title="given-names">B</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">He</span><span class="given-names" tagx="given-names" title="given-names">D</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Liang</span><span class="given-names" tagx="given-names" title="given-names">C</span></span></span><span class="mixed-article-title" title="mixed-article-title">Real-time detection of apple leaf diseases using deep learning approach based on improved convolutional neural networks</span><span class="source" tagx="source" title="source">IEEE Access</span><span class="year" tagx="year" title="year">2019</span><span class="pub-id"><a href="https://dx.doi.org/10.1109/ACCESS.2019.2914929">10.1109/ACCESS.2019.2914929</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/33614364">33614364</a></span></span></li> <li tag="ref"><a name="CR74" /><span class="label" tagx="label" title="label">74.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Long</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Shelhamer</span><span class="given-names" tagx="given-names" title="given-names">E</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Darrell</span><span class="given-names" tagx="given-names" title="given-names">T</span></span></span><span class="mixed-article-title" title="mixed-article-title">Fully convolutional networks for semantic segmentation</span><span class="source" tagx="source" title="source">IEEE Trans Pattern Anal Mach Intell</span><span class="year" tagx="year" title="year">2015</span><span class="volume" tagx="volume" title="volume">39</span><span class="issue" tagx="issue" title="issue">4</span><span class="fpage" tagx="fpage" title="fpage">640</span><span class="lpage" tagx="lpage" title="lpage">651</span></span></li> <li tag="ref"><a name="CR75" /><span class="label" tagx="label" title="label">75.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">He K, Gkioxari G, DollÃ¡r P, Girshick R. Mask R-CNN. In: 2017 IEEE international conference on computer vision (ICCV). New York: IEEE; 2017.</span></li> <li tag="ref"><a name="CR76" /><span class="label" tagx="label" title="label">76.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Ronneberger O, Fischer P, Brox T. U-net: convolutional networks for biomedical image segmentation. In: International conference on medical image computing and computer-assisted intervention. Berlin: Springer; 2015. p. 234â€"41. 10.1007/978-3-319-24574-4_28.</span></li> <li tag="ref"><a name="CR77" /><span class="label" tagx="label" title="label">77.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Badrinarayanan</span><span class="given-names" tagx="given-names" title="given-names">V</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Kendall</span><span class="given-names" tagx="given-names" title="given-names">A</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Cipolla</span><span class="given-names" tagx="given-names" title="given-names">R</span></span></span><span class="mixed-article-title" title="mixed-article-title">Segnet: a deep convolutional encoder-decoder architecture for image segmentation</span><span class="source" tagx="source" title="source">IEEE Trans Pattern Anal Mach Intell</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">39</span><span class="issue" tagx="issue" title="issue">12</span><span class="fpage" tagx="fpage" title="fpage">2481</span><span class="lpage" tagx="lpage" title="lpage">2495</span><span class="pub-id"><a href="https://dx.doi.org/10.1109/TPAMI.2016.2644615">10.1109/TPAMI.2016.2644615</a></span></span></li> <li tag="ref"><a name="CR78" /><span class="label" tagx="label" title="label">78.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wang</span><span class="given-names" tagx="given-names" title="given-names">Z</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Zhang</span><span class="given-names" tagx="given-names" title="given-names">S</span></span></span><span class="mixed-article-title" title="mixed-article-title">Segmentation of corn leaf disease based on fully convolution neural network</span><span class="source" tagx="source" title="source">Acad J Comput Inf Sci</span><span class="year" tagx="year" title="year">2018</span><span class="volume" tagx="volume" title="volume">1</span><span class="fpage" tagx="fpage" title="fpage">9</span><span class="lpage" tagx="lpage" title="lpage">18</span></span></li> <li tag="ref"><a name="CR79" /><span class="label" tagx="label" title="label">79.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wang</span><span class="given-names" tagx="given-names" title="given-names">X</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wang</span><span class="given-names" tagx="given-names" title="given-names">Z</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Zhang</span><span class="given-names" tagx="given-names" title="given-names">S</span></span></span><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Huang</span><span class="given-names" tagx="given-names" title="given-names">DS</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Bevilacqua</span><span class="given-names" tagx="given-names" title="given-names">V</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Premaratne</span><span class="given-names" tagx="given-names" title="given-names">P</span></span></span><span class="mixed-article-title" title="mixed-article-title">Segmenting crop disease leaf image by modified fully-convolutional networks</span><span class="source" tagx="source" title="source">Intelligent computing theories and application. ICIC 2019</span><span class="year" tagx="year" title="year">2019</span><span class="publisher-loc" tagx="publisher-loc" title="publisher-loc">Cham</span><span class="publisher-name" tagx="publisher-name" title="publisher-name">Springer</span></span></li> <li tag="ref"><a name="CR80" /><span class="label" tagx="label" title="label">80.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Lin</span><span class="given-names" tagx="given-names" title="given-names">K</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Gong</span><span class="given-names" tagx="given-names" title="given-names">L</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Huang</span><span class="given-names" tagx="given-names" title="given-names">Y</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Liu</span><span class="given-names" tagx="given-names" title="given-names">C</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Pan</span><span class="given-names" tagx="given-names" title="given-names">J</span></span></span><span class="mixed-article-title" title="mixed-article-title">Deep learning-based segmentation and quantification of cucumber powdery mildew using convolutional neural network</span><span class="source" tagx="source" title="source">Front Plant Sci</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">10</span><span class="fpage" tagx="fpage" title="fpage">155</span><span class="pub-id"><a href="https://dx.doi.org/10.3389/fpls.2019.00155">10.3389/fpls.2019.00155</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/30891048">30891048</a></span></span></li> <li tag="ref"><a name="CR81" /><span class="label" tagx="label" title="label">81.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Kerkech</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Hafiane</span><span class="given-names" tagx="given-names" title="given-names">A</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Canals</span><span class="given-names" tagx="given-names" title="given-names">R</span></span></span><span class="mixed-article-title" title="mixed-article-title">Vine disease detection in UAV multispectral images using optimized image registration and deep learning segmentation approach</span><span class="source" tagx="source" title="source">Comput Electron Agric</span><span class="year" tagx="year" title="year">2020</span><span class="volume" tagx="volume" title="volume">174</span><span class="fpage" tagx="fpage" title="fpage">105446</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.compag.2020.105446">10.1016/j.compag.2020.105446</a></span></span></li> <li tag="ref"><a name="CR82" /><span class="label" tagx="label" title="label">82.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Stewart</span><span class="given-names" tagx="given-names" title="given-names">EL</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wiesner-Hanks</span><span class="given-names" tagx="given-names" title="given-names">T</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Kaczmar</span><span class="given-names" tagx="given-names" title="given-names">N</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Dechant</span><span class="given-names" tagx="given-names" title="given-names">C</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Gore</span><span class="given-names" tagx="given-names" title="given-names">MA</span></span></span><span class="mixed-article-title" title="mixed-article-title">Quantitative phenotyping of northern leaf blight in UAV images using deep learning</span><span class="source" tagx="source" title="source">Remote Sens</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">11</span><span class="issue" tagx="issue" title="issue">19</span><span class="fpage" tagx="fpage" title="fpage">2209</span><span class="pub-id"><a href="https://dx.doi.org/10.3390/rs11192209">10.3390/rs11192209</a></span></span></li> <li tag="ref"><a name="CR83" /><span class="label" tagx="label" title="label">83.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wang</span><span class="given-names" tagx="given-names" title="given-names">Q</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Qi</span><span class="given-names" tagx="given-names" title="given-names">F</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Sun</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Qu</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Xue</span><span class="given-names" tagx="given-names" title="given-names">J</span></span></span><span class="mixed-article-title" title="mixed-article-title">Identification of tomato disease types and detection of infected areas based on deep convolutional neural networks and object detection techniques</span><span class="source" tagx="source" title="source">Comput Intell Neurosci</span><span class="year" tagx="year" title="year">2019</span><span class="pub-id"><a href="https://dx.doi.org/10.1155/2019/9142753">10.1155/2019/9142753</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/31933623">31933623</a></span></span></li> <li tag="ref"><a name="CR84" /><span class="label" tagx="label" title="label">84.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Hughes DP, Salathe M. An open access repository of images on plant health to enable the development of mobile disease diagnostics through machine learning and crowdsourcing. Comput Sci. 2015.</span></li> <li tag="ref"><a name="CR85" /><span class="label" tagx="label" title="label">85.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Shah JP, Prajapati HB, Dabhi VK. A survey on detection and classification of rice plant diseases. In: IEEE international conference on current trends in advanced computing. New York: IEEE; 2016.</span></li> <li tag="ref"><a name="CR86" /><span class="label" tagx="label" title="label">86.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Prajapati</span><span class="given-names" tagx="given-names" title="given-names">HB</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Shah</span><span class="given-names" tagx="given-names" title="given-names">JP</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Dabhi</span><span class="given-names" tagx="given-names" title="given-names">VK</span></span></span><span class="mixed-article-title" title="mixed-article-title">Detection and classification of rice plant diseases</span><span class="source" tagx="source" title="source">Intell Decis Technol</span><span class="year" tagx="year" title="year">2017</span><span class="volume" tagx="volume" title="volume">11</span><span class="issue" tagx="issue" title="issue">3</span><span class="fpage" tagx="fpage" title="fpage">1</span><span class="lpage" tagx="lpage" title="lpage">17</span></span></li> <li tag="ref"><a name="CR87" /><span class="label" tagx="label" title="label">87.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Barbedo</span><span class="given-names" tagx="given-names" title="given-names">JGA</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Koenigkan</span><span class="given-names" tagx="given-names" title="given-names">LV</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Halfeld-Vieira</span><span class="given-names" tagx="given-names" title="given-names">BA</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Costa</span><span class="given-names" tagx="given-names" title="given-names">RV</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Nechet</span><span class="given-names" tagx="given-names" title="given-names">KL</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Godoy</span><span class="given-names" tagx="given-names" title="given-names">CV</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Junior</span><span class="given-names" tagx="given-names" title="given-names">ML</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Patricio</span><span class="given-names" tagx="given-names" title="given-names">FR</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Talamini</span><span class="given-names" tagx="given-names" title="given-names">V</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chitarra</span><span class="given-names" tagx="given-names" title="given-names">LG</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Oliveira</span><span class="given-names" tagx="given-names" title="given-names">SAS</span></span></span><span class="mixed-article-title" title="mixed-article-title">Annotated plant pathology databases for image-based detection and recognition of diseases</span><span class="source" tagx="source" title="source">IEEE Latin Am Trans</span><span class="year" tagx="year" title="year">2018</span><span class="volume" tagx="volume" title="volume">16</span><span class="issue" tagx="issue" title="issue">6</span><span class="fpage" tagx="fpage" title="fpage">1749</span><span class="lpage" tagx="lpage" title="lpage">1757</span><span class="pub-id"><a href="https://dx.doi.org/10.1109/TLA.2018.8444395">10.1109/TLA.2018.8444395</a></span></span></li> <li tag="ref"><a name="CR88" /><span class="label" tagx="label" title="label">88.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Brahimi</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Arsenovic</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Laraba</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Sladojevic</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Boukhalfa</span><span class="given-names" tagx="given-names" title="given-names">K</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Moussaoui</span><span class="given-names" tagx="given-names" title="given-names">A</span></span></span><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Zhou</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chen</span><span class="given-names" tagx="given-names" title="given-names">F</span></span></span><span class="mixed-article-title" title="mixed-article-title">Deep learning for plant diseases: detection and saliency map visualisation</span><span class="source" tagx="source" title="source">Human and machine learning</span><span class="year" tagx="year" title="year">2018</span><span class="publisher-loc" tagx="publisher-loc" title="publisher-loc">Cham</span><span class="publisher-name" tagx="publisher-name" title="publisher-name">Springer</span></span></li> <li tag="ref"><a name="CR89" /><span class="label" tagx="label" title="label">89.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Tyr</span><span class="given-names" tagx="given-names" title="given-names">WH</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Stewart</span><span class="given-names" tagx="given-names" title="given-names">EL</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Nicholas</span><span class="given-names" tagx="given-names" title="given-names">K</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chad</span><span class="given-names" tagx="given-names" title="given-names">DC</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Harvey</span><span class="given-names" tagx="given-names" title="given-names">W</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Nelson</span><span class="given-names" tagx="given-names" title="given-names">RJ</span></span><span class="etal" title="etal"><i>et al.</i></span></span><span class="mixed-article-title" title="mixed-article-title">Image set for deep learning: field images of maize annotated with disease symptoms</span><span class="source" tagx="source" title="source">BMC Res Notes</span><span class="year" tagx="year" title="year">2018</span><span class="volume" tagx="volume" title="volume">11</span><span class="issue" tagx="issue" title="issue">1</span><span class="fpage" tagx="fpage" title="fpage">440</span><span class="pub-id"><a href="https://dx.doi.org/10.1186/s13104-018-3548-6">10.1186/s13104-018-3548-6</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/29970178">29970178</a></span></span></li> <li tag="ref"><a name="CR90" /><span class="label" tagx="label" title="label">90.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Thapa R, Snavely N, Belongie S, Khan A. The plant pathology 2020 challenge dataset to classify foliar disease of apples. arXiv preprint. <a href="http://arxiv.org/abs/2004.11958">arXiv:2004.11958</a>. 2020.</span></li> <li tag="ref"><a name="CR91" /><span class="label" tagx="label" title="label">91.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Wu X, Zhan C, Lai YK, Cheng MM, Yang J. IP102: a large-scale benchmark dataset for insect pest recognition. In: 2019 IEEE/CVF conference on computer vision and pattern recognition (CVPR). New York: IEEE; 2019.</span></li> <li tag="ref"><a name="CR92" /><span class="label" tagx="label" title="label">92.</span><span class="element-citation'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Huang</span><span class="given-names" tagx="given-names" title="given-names">M-L</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chuang</span><span class="given-names" tagx="given-names" title="given-names">TC</span></span><span class="year" tagx="year" title="year">2020</span><div tag="data-title">
A database of eight common tomato pest images</div><span class="source" tagx="source" title="source">Mendeley Data</span><span class="pub-id"><a href="https://dx.doi.org/10.17632/s62zm6djd2.1">10.17632/s62zm6djd2.1</a></span></span></li> <li tag="ref"><a name="CR93" /><span class="label" tagx="label" title="label">93.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde Farley D, Ozair S, Courville A, Bengio Y. Generative adversarial nets. In: Proceedings of the 2014 conference on advances in neural information processing systems 27. Montreal: Curran Associates, Inc.; 2014. p. 2672â€"80.</span></li> <li tag="ref"><a name="CR94" /><span class="label" tagx="label" title="label">94.</span><span class="mixed-citation" tagx="mixed-citation" title="mixed-citation">Pu Y, Gan Z, Henao R, et al. Variational autoencoder for deep learning of images, labels and captions [EB/OL]. 2016â€"09â€"28. <a href="http://arxiv.org/abs/1609.08976">arxiv:1609.08976</a>.</span></li> <li tag="ref"><a name="CR95" /><span class="label" tagx="label" title="label">95.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Oppenheim</span><span class="given-names" tagx="given-names" title="given-names">D</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Shani</span><span class="given-names" tagx="given-names" title="given-names">G</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Erlich</span><span class="given-names" tagx="given-names" title="given-names">O</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Tsror</span><span class="given-names" tagx="given-names" title="given-names">L</span></span></span><span class="mixed-article-title" title="mixed-article-title">Using deep learning for image-based potato tuber disease detection</span><span class="source" tagx="source" title="source">Phytopathology</span><span class="year" tagx="year" title="year">2018</span><span class="volume" tagx="volume" title="volume">109</span><span class="issue" tagx="issue" title="issue">6</span><span class="fpage" tagx="fpage" title="fpage">1083</span><span class="lpage" tagx="lpage" title="lpage">1087</span><span class="pub-id"><a href="https://dx.doi.org/10.1094/PHYTO-08-18-0288-R">10.1094/PHYTO-08-18-0288-R</a></span></span></li> <li tag="ref"><a name="CR96" /><span class="label" tagx="label" title="label">96.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Too</span><span class="given-names" tagx="given-names" title="given-names">EC</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Yujian</span><span class="given-names" tagx="given-names" title="given-names">L</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Njuki</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Yingchun</span><span class="given-names" tagx="given-names" title="given-names">L</span></span></span><span class="mixed-article-title" title="mixed-article-title">A comparative study of fine-tuning deep learning models for plant disease identification</span><span class="source" tagx="source" title="source">Comput Electron Agric</span><span class="year" tagx="year" title="year">2018</span><span class="volume" tagx="volume" title="volume">161</span><span class="fpage" tagx="fpage" title="fpage">272</span><span class="lpage" tagx="lpage" title="lpage">279</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.compag.2018.03.032">10.1016/j.compag.2018.03.032</a></span></span></li> <li tag="ref"><a name="CR97" /><span class="label" tagx="label" title="label">97.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chen</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Chen</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Zhang</span><span class="given-names" tagx="given-names" title="given-names">D</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Sun</span><span class="given-names" tagx="given-names" title="given-names">Y</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Nanehkaran</span><span class="given-names" tagx="given-names" title="given-names">YA</span></span></span><span class="mixed-article-title" title="mixed-article-title">Using deep transfer learning for image-based plant disease identification</span><span class="source" tagx="source" title="source">Comput Electron Agric</span><span class="year" tagx="year" title="year">2020</span><span class="volume" tagx="volume" title="volume">173</span><span class="fpage" tagx="fpage" title="fpage">105393</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.compag.2020.105393">10.1016/j.compag.2020.105393</a></span></span></li> <li tag="ref"><a name="CR98" /><span class="label" tagx="label" title="label">98.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Zhang</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Huang</span><span class="given-names" tagx="given-names" title="given-names">W</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Zhang</span><span class="given-names" tagx="given-names" title="given-names">C</span></span></span><span class="mixed-article-title" title="mixed-article-title">Three-channel convolutional neural networks for vegetable leaf disease recognition</span><span class="source" tagx="source" title="source">Cogn Syst Res</span><span class="year" tagx="year" title="year">2018</span><span class="volume" tagx="volume" title="volume">53</span><span class="fpage" tagx="fpage" title="fpage">31</span><span class="lpage" tagx="lpage" title="lpage">41</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.cogsys.2018.04.006">10.1016/j.cogsys.2018.04.006</a></span></span></li> <li tag="ref"><a name="CR99" /><span class="label" tagx="label" title="label">99.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Liu</span><span class="given-names" tagx="given-names" title="given-names">B</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Ding</span><span class="given-names" tagx="given-names" title="given-names">Z</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Tian</span><span class="given-names" tagx="given-names" title="given-names">L</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">He</span><span class="given-names" tagx="given-names" title="given-names">D</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Li</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wang</span><span class="given-names" tagx="given-names" title="given-names">H</span></span></span><span class="mixed-article-title" title="mixed-article-title">Grape leaf disease identification using improved deep convolutional neural networks</span><span class="source" tagx="source" title="source">Front Plant Sci</span><span class="year" tagx="year" title="year">2020</span><span class="volume" tagx="volume" title="volume">11</span><span class="fpage" tagx="fpage" title="fpage">1082</span><span class="pub-id"><a href="https://dx.doi.org/10.3389/fpls.2020.01082">10.3389/fpls.2020.01082</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/32760419">32760419</a></span></span></li> <li tag="ref"><a name="CR100" /><span class="label" tagx="label" title="label">100.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Karthik</span><span class="given-names" tagx="given-names" title="given-names">R</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Hariharan</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Anand</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="etal" title="etal"><i>et al.</i></span></span><span class="mixed-article-title" title="mixed-article-title">Attention embedded residual CNN for disease detection in tomato leaves</span><span class="source" tagx="source" title="source">Appl Soft Comput J.</span><span class="year" tagx="year" title="year">2020</span><span class="volume" tagx="volume" title="volume">86</span><span class="fpage" tagx="fpage" title="fpage">105933</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.asoc.2019.105933">10.1016/j.asoc.2019.105933</a></span></span></li> <li tag="ref"><a name="CR101" /><span class="label" tagx="label" title="label">101.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Guan</span><span class="given-names" tagx="given-names" title="given-names">W</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Yu</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Jianxin</span><span class="given-names" tagx="given-names" title="given-names">W</span></span></span><span class="mixed-article-title" title="mixed-article-title">Automatic image-based plant disease severity estimation using deep learning</span><span class="source" tagx="source" title="source">Comput Intell Neurosci</span><span class="year" tagx="year" title="year">2017</span><span class="volume" tagx="volume" title="volume">2017</span><span class="fpage" tagx="fpage" title="fpage">2917536</span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/28757863">28757863</a></span></span></li> <li tag="ref"><a name="CR102" /><span class="label" tagx="label" title="label">102.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Barbedo</span><span class="given-names" tagx="given-names" title="given-names">JGA</span></span></span><span class="mixed-article-title" title="mixed-article-title">Factors influencing the use of deep learning for plant disease recognition</span><span class="source" tagx="source" title="source">Biosyst Eng</span><span class="year" tagx="year" title="year">2018</span><span class="volume" tagx="volume" title="volume">172</span><span class="fpage" tagx="fpage" title="fpage">84</span><span class="lpage" tagx="lpage" title="lpage">91</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.biosystemseng.2018.05.013">10.1016/j.biosystemseng.2018.05.013</a></span></span></li> <li tag="ref"><a name="CR103" /><span class="label" tagx="label" title="label">103.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Barbedo</span><span class="given-names" tagx="given-names" title="given-names">JGA</span></span></span><span class="mixed-article-title" title="mixed-article-title">Impact of dataset size and variety on the effectiveness of deep learning and transfer learning for plant disease classification</span><span class="source" tagx="source" title="source">Comput Electron Agric</span><span class="year" tagx="year" title="year">2018</span><span class="volume" tagx="volume" title="volume">153</span><span class="fpage" tagx="fpage" title="fpage">46</span><span class="lpage" tagx="lpage" title="lpage">53</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.compag.2018.08.013">10.1016/j.compag.2018.08.013</a></span></span></li> <li tag="ref"><a name="CR104" /><span class="label" tagx="label" title="label">104.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Nawaz</span><span class="given-names" tagx="given-names" title="given-names">MA</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Khan</span><span class="given-names" tagx="given-names" title="given-names">T</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Mudassar</span><span class="given-names" tagx="given-names" title="given-names">R</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Kausar</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Ahmad</span><span class="given-names" tagx="given-names" title="given-names">J</span></span></span><span class="mixed-article-title" title="mixed-article-title">Plant disease detection using internet of thing (IOT)</span><span class="source" tagx="source" title="source">Int J Adv Comput Sci Appl</span><span class="year" tagx="year" title="year">2020</span><span class="pub-id"><a href="https://dx.doi.org/10.14569/IJACSA.2020.0110162">10.14569/IJACSA.2020.0110162</a></span></span></li> <li tag="ref"><a name="CR105" /><span class="label" tagx="label" title="label">105.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Martinelli</span><span class="given-names" tagx="given-names" title="given-names">F</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Scalenghe</span><span class="given-names" tagx="given-names" title="given-names">R</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Davino</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Panno</span><span class="given-names" tagx="given-names" title="given-names">S</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Scuderi</span><span class="given-names" tagx="given-names" title="given-names">G</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Ruisi</span><span class="given-names" tagx="given-names" title="given-names">P</span></span><span class="etal" title="etal"><i>et al.</i></span></span><span class="mixed-article-title" title="mixed-article-title">Advanced methods of plant disease detection. A review</span><span class="source" tagx="source" title="source">Agron Sustain Dev</span><span class="year" tagx="year" title="year">2015</span><span class="volume" tagx="volume" title="volume">35</span><span class="issue" tagx="issue" title="issue">1</span><span class="fpage" tagx="fpage" title="fpage">1</span><span class="lpage" tagx="lpage" title="lpage">25</span><span class="pub-id"><a href="https://dx.doi.org/10.1007/s13593-014-0246-1">10.1007/s13593-014-0246-1</a></span></span></li> <li tag="ref"><a name="CR106" /><span class="label" tagx="label" title="label">106.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Liu</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wang</span><span class="given-names" tagx="given-names" title="given-names">X</span></span></span><span class="mixed-article-title" title="mixed-article-title">Early recognition of tomato gray leaf spot disease based on MobileNetv2-YOLOv3 model</span><span class="source" tagx="source" title="source">Plant Methods</span><span class="year" tagx="year" title="year">2020</span><span class="volume" tagx="volume" title="volume">16</span><span class="fpage" tagx="fpage" title="fpage">83</span><span class="pub-id"><a href="https://dx.doi.org/10.1186/s13007-020-00624-2">10.1186/s13007-020-00624-2</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/33563319">33563319</a></span></span></li> <li tag="ref"><a name="CR107" /><span class="label" tagx="label" title="label">107.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Liu</span><span class="given-names" tagx="given-names" title="given-names">J</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wang</span><span class="given-names" tagx="given-names" title="given-names">X</span></span></span><span class="mixed-article-title" title="mixed-article-title">Tomato diseases and pests detection based on improved Yolo V3 convolutional neural network</span><span class="source" tagx="source" title="source">Front Plant Sci</span><span class="year" tagx="year" title="year">2020</span><span class="volume" tagx="volume" title="volume">11</span><span class="fpage" tagx="fpage" title="fpage">898</span><span class="pub-id"><a href="https://dx.doi.org/10.3389/fpls.2020.00898">10.3389/fpls.2020.00898</a></span><span class="pub-id"><a href="http://www.ncbi.nlm.nih.gov/pubmed/32612632">32612632</a></span></span></li> <li tag="ref"><a name="CR108" /><span class="label" tagx="label" title="label">108.</span><span class="element-citation'"><span class="person-group'"><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Kamal</span><span class="given-names" tagx="given-names" title="given-names">KC</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Yin</span><span class="given-names" tagx="given-names" title="given-names">Z</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wu</span><span class="given-names" tagx="given-names" title="given-names">M</span></span><span class="name" tagx="name" title="name"><span class="surname" tagx="surname" title="surname">Wu</span><span class="given-names" tagx="given-names" title="given-names">Z</span></span></span><span class="mixed-article-title" title="mixed-article-title">Depthwise separable convolution architectures for plant disease classification</span><span class="source" tagx="source" title="source">Comput Electron Agric</span><span class="year" tagx="year" title="year">2019</span><span class="volume" tagx="volume" title="volume">165</span><span class="fpage" tagx="fpage" title="fpage">104948</span><span class="pub-id"><a href="https://dx.doi.org/10.1016/j.compag.2019.104948">10.1016/j.compag.2019.104948</a></span></span></li> </ul> </div> </div>  </body></html>